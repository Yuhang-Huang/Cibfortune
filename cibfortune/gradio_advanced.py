#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-VL-8B-Instruct é«˜çº§Gradioç•Œé¢
åŒ…å«æ›´å¤šé«˜çº§åŠŸèƒ½å’Œæ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
"""

import os
import gradio as gr
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from PIL import Image
import requests
from io import BytesIO
import time
import json
import base64
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class AdvancedQwen3VLApp:
    """é«˜çº§Qwen3-VLåº”ç”¨ç±»"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.model_path = "/data/storage1/wulin/models/qwen3-vl-8b-instruct"
        self.is_loaded = False
        self.chat_history = []
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def load_model(self, progress=gr.Progress()):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            return "âœ… æ¨¡å‹å·²ç»åŠ è½½å®Œæˆï¼", gr.update(interactive=True)
        
        try:
            progress(0.1, desc="æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
            if not os.path.exists(self.model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}", gr.update(interactive=False)
            
            progress(0.3, desc="åŠ è½½æ¨¡å‹...")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype="auto",
                device_map="auto"
            )
            
            progress(0.7, desc="åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            
            progress(1.0, desc="å®Œæˆï¼")
            self.is_loaded = True
            
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚", gr.update(interactive=True)
            
        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", gr.update(interactive=False)
    
    def chat_with_image(self, image, text, history, max_tokens, temperature, top_p, top_k, repetition_penalty: float = 1.0, presence_penalty: float = 1.5):
        """ä¸å›¾åƒå¯¹è¯"""
        if not self.is_loaded:
            return history, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼", ""
        
        if image is None:
            return history, "âŒ è¯·ä¸Šä¼ å›¾åƒï¼", ""
        
        if not text.strip():
            return history, "âŒ è¯·è¾“å…¥é—®é¢˜ï¼", ""
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": text},
                    ],
                }
            ]
            
            # å‡†å¤‡è¾“å…¥
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            # ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "do_sample": True if temperature > 0 else False,
                "repetition_penalty": repetition_penalty
                # presence_penalty å‚æ•°ä¸º OpenAI é£æ ¼ï¼ŒTransformers ä¸åŸç”Ÿæ”¯æŒï¼Œæ­¤å¤„ä¿ç•™å ä½
            }
            
            # ç”Ÿæˆå›ç­”
            start_time = time.time()
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, **generation_kwargs)
            
            generation_time = time.time() - start_time
            
            # å¤„ç†è¾“å‡º
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            response = output_text[0]
            
            # æ›´æ–°å†å²è®°å½•
            history.append([f"ğŸ‘¤ {text}", f"ğŸ¤– {response}"])
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = f"â±ï¸ ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’ | ğŸ“ ç”Ÿæˆé•¿åº¦: {len(response)}å­—ç¬¦"
            
            return history, "", stats
            
        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            history.append([f"ğŸ‘¤ {text}", error_msg])
            return history, "", f"âŒ é”™è¯¯: {str(e)}"
    
    def batch_analysis(self, images, analysis_type):
        """æ‰¹é‡åˆ†æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if not images:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
        
        results = []
        
        for i, image in enumerate(images):
            try:
                if analysis_type == "æè¿°":
                    prompt = "è¯·çœŸå®ã€è¯¦ç»†ã€å®¢è§‚åœ°æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
                elif analysis_type == "OCR":
                    prompt = "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œå°½é‡è¿˜åŸåŸæœ¬æ ·å¼ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚"
                elif analysis_type == "ç©ºé—´åˆ†æ":
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»å’Œç‰©ä½“ä½ç½®ï¼ŒåŒ…æ‹¬ç›¸å¯¹ä½ç½®ã€è§†è§’ã€é®æŒ¡ã€æ·±åº¦ä¸è·ç¦»æ„Ÿï¼Œå¹¶ç»™å‡ºæ•´ä½“å¸ƒå±€æè¿°ã€‚"
                elif analysis_type == "æƒ…æ„Ÿåˆ†æ":
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæˆ–æ°›å›´ã€‚"
                else:
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ã€‚"
                
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": prompt},
                        ],
                    }
                ]
                
                inputs = self.processor.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt"
                )
                inputs = inputs.to(self.model.device)
                
                with torch.no_grad():
                    generated_ids = self.model.generate(**inputs, max_new_tokens=512)
                
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = self.processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )
                
                results.append(f"ğŸ“· å›¾åƒ {i+1}:\n{output_text[0]}\n" + "="*50 + "\n")
                
            except Exception as e:
                results.append(f"ğŸ“· å›¾åƒ {i+1}: âŒ åˆ†æå¤±è´¥ - {str(e)}\n" + "="*50 + "\n")
        
        return "".join(results)
    
    def compare_images(self, image1, image2, comparison_type):
        """å›¾åƒå¯¹æ¯”"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if image1 is None or image2 is None:
            return "âŒ è¯·ä¸Šä¼ ä¸¤å¼ å›¾åƒè¿›è¡Œå¯¹æ¯”ï¼"
        
        try:
            if comparison_type == "ç›¸ä¼¼æ€§":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œåˆ†æå®ƒä»¬çš„ç›¸ä¼¼ä¹‹å¤„å’Œä¸åŒä¹‹å¤„ã€‚"
            elif comparison_type == "é£æ ¼":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡çš„è‰ºæœ¯é£æ ¼ã€è‰²å½©æ­é…å’Œæ„å›¾ç‰¹ç‚¹ã€‚"
            elif comparison_type == "å†…å®¹":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œåˆ†æå®ƒä»¬æè¿°çš„åœºæ™¯æˆ–ä¸»é¢˜ã€‚"
            else:
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œæä¾›è¯¦ç»†çš„å¯¹æ¯”åˆ†æã€‚"
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image1},
                        {"type": "image", "image": image2},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            return f"ğŸ” å¯¹æ¯”åˆ†æç»“æœ:\n\n{output_text[0]}"
            
        except Exception as e:
            return f"âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}"
    
    def export_chat_history(self):
        """å¯¼å‡ºå¯¹è¯å†å²"""
        if not self.chat_history:
            return "âŒ æ²¡æœ‰å¯¹è¯å†å²å¯å¯¼å‡ºï¼"
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_history_{timestamp}.json"
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.chat_history, f, ensure_ascii=False, indent=2)
            
            return f"âœ… å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°: {filename}"
            
        except Exception as e:
            return f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}"
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
        return []

# åˆ›å»ºåº”ç”¨å®ä¾‹
app = AdvancedQwen3VLApp()

def create_advanced_interface():
    """åˆ›å»ºé«˜çº§Gradioç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen3-VL-8B-Instruct é«˜çº§ç•Œé¢",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
        }
        .chat-message {
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
        }
        .stats-box {
            background-color: #f0f0f0;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
        """
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¤– å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½åˆ†æåŠ©æ‰‹
        
        **åŠŸèƒ½ç‰¹è‰²ï¼š**
        - ğŸ–¼ï¸ æ™ºèƒ½å›¾åƒç†è§£å’Œå¯¹è¯ \t ğŸ“ å¤šè¯­è¨€OCRè¯†åˆ«
        - ğŸ” ç©ºé—´æ„ŸçŸ¥å’Œæƒ…æ„Ÿåˆ†æ \t ğŸ’» è§†è§‰ç¼–ç¨‹ä»£ç ç”Ÿæˆ
        - ğŸ“Š æ‰¹é‡å›¾åƒå¤„ç† \t ğŸ”„ å›¾åƒå¯¹æ¯”åˆ†æ
        - ğŸ’¾ å¯¹è¯å†å²å¯¼å‡º \t ğŸ“– ä½¿ç”¨è¯´æ˜
        """)
        
        with gr.Tab("ğŸš€ æ¨¡å‹ç®¡ç†"):
            gr.Markdown("### æ¨¡å‹åŠ è½½ä¸ç®¡ç†")
            with gr.Row():
                with gr.Column():
                    load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
                    status_text = gr.Textbox(
                        label="çŠ¶æ€", 
                        value="â³ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç‚¹å‡»åŠ è½½æ¨¡å‹æŒ‰é’®",
                        interactive=False
                    )
                with gr.Column():
                    model_info = gr.Textbox(
                        label="æ¨¡å‹ä¿¡æ¯",
                        value=f"æ¨¡å‹è·¯å¾„: {app.model_path}",
                        interactive=False
                    )
            
            load_btn.click(
                app.load_model,
                outputs=[status_text, load_btn]
            )
        
        with gr.Tab("ğŸ’¬ æ™ºèƒ½å¯¹è¯"):
            gr.Markdown("### ä¸å›¾åƒè¿›è¡Œæ™ºèƒ½å¯¹è¯")
            
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ å›¾åƒ",
                        type="pil",
                        height=400
                    )
                    
                    with gr.Accordion("ğŸ›ï¸ ç”Ÿæˆå‚æ•°", open=False):
                        max_tokens = gr.Slider(
                            minimum=50, maximum=2048, value=256,
                            label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                        )
                        temperature = gr.Slider(
                            minimum=0.1, maximum=2.0, value=0.7,
                            label="åˆ›é€ æ€§ (Temperature)"
                        )
                        top_p = gr.Slider(
                            minimum=0.1, maximum=1.0, value=0.8,
                            label="Top-p"
                        )
                        top_k = gr.Slider(
                            minimum=1, maximum=100, value=20,
                            label="Top-k"
                        )
                
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=400,
                        show_label=True
                    )
                    
                    with gr.Row():
                        text_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            placeholder="è¯·æè¿°è¿™å¼ å›¾ç‰‡...",
                            lines=2
                        )
                        send_btn = gr.Button("å‘é€", variant="primary")
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²")
                        export_btn = gr.Button("ğŸ’¾ å¯¼å‡ºå†å²")
                    
                    stats_output = gr.Textbox(
                        label="ç”Ÿæˆç»Ÿè®¡",
                        interactive=False
                    )
            
            # äº‹ä»¶ç»‘å®š
            send_btn.click(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature, top_p, top_k],
                outputs=[chatbot, text_input, stats_output]
            )
            
            text_input.submit(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature, top_p, top_k],
                outputs=[chatbot, text_input, stats_output]
            )
            
            clear_btn.click(
                app.clear_history,
                outputs=[chatbot]
            )
            
            export_btn.click(
                app.export_chat_history,
                outputs=[stats_output]
            )
        
        with gr.Tab("ğŸ“Š æ‰¹é‡åˆ†æ"):
            gr.Markdown("### æ‰¹é‡å›¾åƒåˆ†æ")
            
            with gr.Row():
                with gr.Column():
                    batch_images = gr.File(
                        label="ä¸Šä¼ å¤šä¸ªå›¾åƒ",
                        file_count="multiple",
                        file_types=["image"]
                    )
                    
                    analysis_type = gr.Dropdown(
                        choices=["æè¿°", "OCR", "ç©ºé—´åˆ†æ", "æƒ…æ„Ÿåˆ†æ"],
                        value="æè¿°",
                        label="åˆ†æç±»å‹"
                    )
                    
                    batch_btn = gr.Button("ğŸ” å¼€å§‹æ‰¹é‡åˆ†æ", variant="primary")
                
                with gr.Column():
                    batch_result = gr.Textbox(
                        label="æ‰¹é‡åˆ†æç»“æœ",
                        lines=20,
                        max_lines=30
                    )
            
            batch_btn.click(
                app.batch_analysis,
                inputs=[batch_images, analysis_type],
                outputs=[batch_result]
            )
        
        with gr.Tab("ğŸ”„ å›¾åƒå¯¹æ¯”"):
            gr.Markdown("### å›¾åƒå¯¹æ¯”åˆ†æ")
            
            with gr.Row():
                with gr.Column():
                    compare_image1 = gr.Image(
                        label="å›¾åƒ1",
                        type="pil",
                        height=200
                    )
                    compare_image2 = gr.Image(
                        label="å›¾åƒ2", 
                        type="pil",
                        height=200
                    )
                    
                    comparison_type = gr.Dropdown(
                        choices=["ç›¸ä¼¼æ€§", "é£æ ¼", "å†…å®¹", "ç»¼åˆ"],
                        value="ç›¸ä¼¼æ€§",
                        label="å¯¹æ¯”ç±»å‹"
                    )
                    
                    compare_btn = gr.Button("ğŸ”„ å¼€å§‹å¯¹æ¯”", variant="primary")
                
                with gr.Column():
                    compare_result = gr.Textbox(
                        label="å¯¹æ¯”ç»“æœ",
                        lines=20,
                        max_lines=25
                    )
            
            compare_btn.click(
                app.compare_images,
                inputs=[compare_image1, compare_image2, comparison_type],
                outputs=[compare_result]
            )
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜
            
            ### ğŸš€ æ¨¡å‹ç®¡ç†
            - **åŠ è½½æ¨¡å‹**: é¦–æ¬¡ä½¿ç”¨å¿…é¡»ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
            - **æ¨¡å‹è·¯å¾„**: `/data/storage1/wulin/models/qwen3-vl-8b-instruct`
            - **åŠ è½½æ—¶é—´**: é€šå¸¸éœ€è¦10ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…
            
            ### ğŸ’¬ æ™ºèƒ½å¯¹è¯
            - **å›¾åƒä¸Šä¼ **: æ”¯æŒJPGã€PNGç­‰å¸¸è§æ ¼å¼
            - **å‚æ•°è°ƒèŠ‚**: 
              - æœ€å¤§ç”Ÿæˆé•¿åº¦: æ§åˆ¶å›ç­”çš„è¯¦ç»†ç¨‹åº¦
              - åˆ›é€ æ€§: æ•°å€¼è¶Šé«˜å›ç­”è¶Šæœ‰åˆ›æ„
              - Top-p/Top-k: æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
            - **å¤šè½®å¯¹è¯**: æ”¯æŒåŸºäºå›¾åƒçš„è¿ç»­å¯¹è¯
            - **å†å²ç®¡ç†**: å¯æ¸…ç©ºæˆ–å¯¼å‡ºå¯¹è¯å†å²
            
            ### ğŸ“Š æ‰¹é‡åˆ†æ
            - **å¤šå›¾åƒä¸Šä¼ **: ä¸€æ¬¡å¯ä¸Šä¼ å¤šå¼ å›¾åƒ
            - **åˆ†æç±»å‹**:
              - æè¿°: è¯¦ç»†æè¿°å›¾åƒå†…å®¹
              - OCR: æå–å›¾åƒä¸­çš„æ–‡å­—
              - ç©ºé—´åˆ†æ: åˆ†æç©ºé—´å…³ç³»
              - æƒ…æ„Ÿåˆ†æ: åˆ†æå›¾åƒæƒ…æ„Ÿæ°›å›´
            
            ### ğŸ”„ å›¾åƒå¯¹æ¯”
            - **å¯¹æ¯”ç±»å‹**:
              - ç›¸ä¼¼æ€§: åˆ†æå›¾åƒçš„ç›¸ä¼¼å’Œå·®å¼‚
              - é£æ ¼: å¯¹æ¯”è‰ºæœ¯é£æ ¼å’Œè‰²å½©
              - å†…å®¹: å¯¹æ¯”åœºæ™¯å’Œä¸»é¢˜
              - ç»¼åˆ: å…¨é¢çš„å¯¹æ¯”åˆ†æ
            
            ### âš ï¸ æ³¨æ„äº‹é¡¹
            - ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼ˆå»ºè®®16GB+ï¼‰
            - æ”¯æŒGPUåŠ é€Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
            - å¤§å›¾åƒå¯èƒ½éœ€è¦æ›´é•¿çš„å¤„ç†æ—¶é—´
            - å»ºè®®ä¸€æ¬¡å¤„ç†ä¸è¶…è¿‡10å¼ å›¾åƒ
            """)
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Qwen3-VL-8B-Instruct é«˜çº§Webç•Œé¢...")
    
    # åˆ›å»ºç•Œé¢
    interface = create_advanced_interface()
    
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",
        server_port=7861,  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
        share=False,
        debug=True,
        show_error=True
    )

if __name__ == "__main__":
    main()
