#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-VL-8B-Instruct ç»Ÿä¸€Gradioç•Œé¢
æ”¯æŒåœ¨åŒä¸€ç•Œé¢å†…åˆ‡æ¢ã€Œé€šç”¨ç‰ˆã€ä¸ã€Œä¸“ä¸šç‰ˆã€ï¼Œå¹¶æä¾›è§¦å±å‹å¥½æ ·å¼
"""

import os
import json
import inspect
import io
import hashlib
import time
import csv
import html
import numpy as np
from datetime import datetime
import shutil
import atexit
import gc

import gradio as gr
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from ocr_card_rag_api import CardOCRWithRAG

try:
    import torch
except Exception:
    torch = None

# ç»Ÿä¸€ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class AdvancedQwen3VLApp:
    """é«˜çº§Qwen3-VLåº”ç”¨ç±»"""

    def __init__(self):
        self.model = None
        self.processor = None
        """D:\cibfortune\Cibfortune\cibfortune\models\qwen3-vl-2b-instruct"""
        self.model_path = "/data/storage1/wulin/models/qwen3-vl-8b-instruct"
        self.is_loaded = False
        self.chat_history = []
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.chat_messages = []
        self.last_image = None
        self.last_saved_image_path = None
        self.last_image_digest = None
        self.last_ocr_markdown = None
        self.last_ocr_html = None
        # å¡è¯OCRå¤šæ¨¡æ€RAGç»„ä»¶
        self.card_rag_store = None
        self.card_rag_ready = False
        self.card_rag_dir = "rag_cards"
        # API å¡è¯OCRï¼ˆRAG + Qwen APIï¼‰
        self.card_api = None
<<<<<<< Updated upstream
        # å­—æ®µæ¨¡æ¿ç›®å½•
        self.field_templates_dir = "card_field_templates"
=======
        self.card_api_feature_mode = "clip"
>>>>>>> Stashed changes

    def _ensure_card_rag_loaded(self):
        """æ‡’åŠ è½½å¡è¯RAGå›¾ç‰‡åº“ï¼ˆè‹¥å­˜åœ¨ rag_cards ç›®å½•ï¼‰ï¼Œæ”¯æŒå¤šç§RAGå®ç°æ–¹å¼ã€‚"""
        if self.card_rag_ready:
            return
        try:
            if not os.path.isdir(self.card_rag_dir):
                self.card_rag_ready = True  # æ ‡è®°ä¸ºå·²å°è¯•ï¼Œé¿å…é‡å¤æ£€æŸ¥
                return
            
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨ multimodal_rag æ¨¡å—
            try:
                from multimodal_rag import MultiModalDocumentLoader, MultiModalVectorStore
                loader = MultiModalDocumentLoader()
                docs = loader.load_images_from_folder(self.card_rag_dir)
                if not docs:
                    self.card_rag_ready = True
                    return
                store = MultiModalVectorStore(persist_directory="./multimodal_chroma_card")
                store.create_vector_store(docs)
                self.card_rag_store = store
                self.card_rag_ready = True
                print("âœ… ä½¿ç”¨multimodal_ragåŠ è½½RAGå›¾ç‰‡åº“æˆåŠŸ")
                return
            except Exception as e:
                print(f"âš ï¸ ä½¿ç”¨multimodal_ragåŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–ç‰ˆRAG")
            
            # å¦‚æœmultimodal_ragä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨SimpleRAGStoreï¼ˆä»ocr_card_rag_apiå¯¼å…¥ï¼‰
            try:
                from ocr_card_rag_api import SimpleRAGStore
                print("ä½¿ç”¨ç®€åŒ–ç‰ˆRAGåŠŸèƒ½ï¼ˆåŸºäºå¡é¢æ ·å¼ç‰¹å¾ï¼‰...")
                store = SimpleRAGStore(use_style_features=True)
                store.load_images_from_folder(self.card_rag_dir)
                
                if not store.image_embeddings:
                    print("âš ï¸ RAGå›¾ç‰‡åº“ä¸ºç©º")
                    self.card_rag_ready = True
                    return False
                
                self.card_rag_store = store
                self.card_rag_ready = True
                print(f"âœ… ä½¿ç”¨ç®€åŒ–ç‰ˆRAGåŠ è½½æˆåŠŸï¼Œå…± {len(store.image_embeddings)} å¼ å›¾ç‰‡")
                return
            except Exception as e:
                print(f"âš ï¸ ä½¿ç”¨ç®€åŒ–ç‰ˆRAGåŠ è½½å¤±è´¥: {e}")
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œæ ‡è®°ä¸ºå·²å°è¯•
            self.card_rag_ready = True
        except Exception as e:
            print(f"åŠ è½½RAGå›¾ç‰‡åº“å¤±è´¥: {e}")
            self.card_rag_ready = True

    def _ensure_card_api_loaded(self):
        """æ‡’åŠ è½½å¡è¯OCRï¼ˆæ”¯æŒ åœ¨çº¿APIæ¨¡å¼ + ç¦»çº¿RAGæ¨¡å¼ï¼‰"""
        if self.card_api is not None:
            return

        try:
            # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å¯ç”¨ APIï¼šç¯å¢ƒå˜é‡ä¸­æ‰¾ key
            env_key = os.environ.get("QWEN_API_KEY") or os.environ.get("OPENAI_API_KEY")
            has_api_key = bool(env_key)

            # åˆ¤æ–­å½“å‰æ˜¯å¦æ˜¯æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æ— éœ€è°ƒç”¨ APIï¼‰
            is_local_model = isinstance(self.model_path, str) and os.path.isdir(self.model_path)

            # å†³ç­–ï¼šåªè¦æœ¬åœ°æ¨¡å‹ or æ—  key â†’ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            use_api = has_api_key and (not is_local_model)

            api = CardOCRWithRAG(
                api_key=env_key if use_api else None,
                model="qwen-vl-plus" if use_api else "local-offline",
                rag_image_dir=self.card_rag_dir,
                persist_directory="./multimodal_chroma_card",
                rag_feature_mode=self.card_api_feature_mode,
                use_api=use_api,   # â­ å†³å®šæ˜¯å¦è°ƒç”¨ API
            )

            # åŠ è½½æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ä¸ä¼šåˆå§‹åŒ– OpenAI clientï¼‰
            api.load_model()

            # åŠ è½½ RAG å›¾ç‰‡åº“
            api.load_rag_library()

            self.card_api = api

            mode_str = "åœ¨çº¿APIæ¨¡å¼" if use_api else "ç¦»çº¿RAGæ¨¡å¼"
            print(f"ğŸŸ© å¡è¯OCR å·²åˆå§‹åŒ–ï¼ˆ{mode_str}ï¼‰")

        except Exception as e:
            print(f"âŒ å¡è¯OCRåˆå§‹åŒ–å¤±è´¥: {e}")
            self.card_api = None

    def set_card_api_feature_mode(self, selection: str):
        """æ›´æ–°APIç‰ˆå¡è¯OCRæ‰€ä½¿ç”¨çš„RAGç‰¹å¾æ¨¡å¼ã€‚"""
        normalized = "clip" if selection and "clip" in selection.lower() else "style"
        if normalized != self.card_api_feature_mode:
            self.card_api_feature_mode = normalized
            # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œä½¿æ–°è®¾ç½®ç”Ÿæ•ˆ
            self.card_api = None

    def _rag_search_card(self, image, top_k: int = 3):
        """
        å¯¹è¾“å…¥å›¾ç‰‡è¿›è¡ŒRAGæ£€ç´¢ï¼Œè¿”å›ç›¸ä¼¼å›¾ç‰‡ä¿¡æ¯ï¼ˆä¸ocr_card_rag_api.pyä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
        
        Args:
            image: è¾“å…¥å›¾ç‰‡ï¼ˆPIL Imageï¼‰
            top_k: è¿”å›æœ€ç›¸ä¼¼çš„kå¼ å›¾ç‰‡
            
        Returns:
            ç›¸ä¼¼å›¾ç‰‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {filename, similarity, metadata}
        """
        if not self.card_rag_store or not hasattr(self.card_rag_store, "image_embeddings"):
            return []
            
        try:
            # ç”ŸæˆæŸ¥è¯¢å›¾ç‰‡çš„åµŒå…¥å‘é‡
            # å…¼å®¹ä¸¤ç§å®ç°ï¼šMultiModalVectorStore ä½¿ç”¨ .embeddings.embed_imageï¼ŒSimpleRAGStore ç›´æ¥ä½¿ç”¨ .embed_image
            if hasattr(self.card_rag_store, "embeddings") and hasattr(self.card_rag_store.embeddings, "embed_image"):
                # ä½¿ç”¨ MultiModalVectorStore
                query_emb = self.card_rag_store.embeddings.embed_image(image)
            elif hasattr(self.card_rag_store, "embed_image"):
                # ä½¿ç”¨ SimpleRAGStore
                query_emb = self.card_rag_store.embed_image(image)
            else:
                print("âš ï¸ RAGå­˜å‚¨ä¸æ”¯æŒembed_imageæ–¹æ³•")
                return []
            
            # è®¡ç®—ä¸å›¾ç‰‡åº“ä¸­æ‰€æœ‰å›¾ç‰‡çš„ç›¸ä¼¼åº¦
            similarities = []
            # å¦‚æœSimpleRAGStoreæœ‰compute_similarityæ–¹æ³•ï¼Œä½¿ç”¨å®ƒï¼ˆæ”¯æŒæ ·å¼ç›¸ä¼¼åº¦ï¼‰
            use_compute_similarity = hasattr(self.card_rag_store, "compute_similarity")
            
            # ç¡®ä¿æŸ¥è¯¢å‘é‡çš„ç»´åº¦
            query_dim = len(query_emb) if hasattr(query_emb, '__len__') else query_emb.shape[0] if hasattr(query_emb, 'shape') else 0
            
            for idx, emb in enumerate(self.card_rag_store.image_embeddings):
                try:
                    # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                    emb_dim = len(emb) if hasattr(emb, '__len__') else emb.shape[0] if hasattr(emb, 'shape') else 0
                    
                    if query_dim != emb_dim:
                        # ç»´åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡æˆ–ä½¿ç”¨é»˜è®¤ç›¸ä¼¼åº¦
                        print(f"âš ï¸ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æŸ¥è¯¢å‘é‡={query_dim}, å›¾ç‰‡åº“å‘é‡={emb_dim}ï¼Œè·³è¿‡è¯¥å›¾ç‰‡")
                        continue
                    
                    if use_compute_similarity:
                        # ä½¿ç”¨æ ·å¼ç›¸ä¼¼åº¦æˆ–CLIPç›¸ä¼¼åº¦ï¼ˆæ ¹æ®SimpleRAGStoreçš„é…ç½®ï¼‰
                        similarity = self.card_rag_store.compute_similarity(query_emb, emb)
                    else:
                        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆMultiModalVectorStoreï¼‰
                        dot_product = np.dot(query_emb, emb)
                        norm_query = np.linalg.norm(query_emb)
                        norm_emb = np.linalg.norm(emb)
                        denom = norm_query * norm_emb + 1e-8
                        similarity = float(dot_product / denom) if denom > 0 else 0.0
                    similarities.append((similarity, idx))
                except Exception as e:
                    # å¦‚æœè®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™ï¼Œè·³è¿‡è¯¥å›¾ç‰‡
                    print(f"âš ï¸ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ï¼ˆå›¾ç‰‡{idx}ï¼‰: {str(e)}")
                    continue
            
            # æ’åºå¹¶å–Top-K
            similarities.sort(key=lambda x: x[0], reverse=True)
            top_results = []
            
            for sim, idx in similarities[:top_k]:
                if idx < len(self.card_rag_store.image_metadatas):
                    meta = self.card_rag_store.image_metadatas[idx]
                    filename = meta.get("filename") or os.path.basename(meta.get("source", "")) or f"å›¾ç‰‡{idx+1}"
                    top_results.append({
                        "filename": filename,
                        "similarity": sim,
                        "metadata": meta
                    })
                    
            return top_results
            
        except Exception as e:
            print(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥: {str(e)}")
            return []

    def _build_enhanced_prompt_card(self, base_prompt: str, rag_results: list, custom_prompt: str = None):
        """
        æ„å»ºå¢å¼ºåçš„æç¤ºè¯ï¼ˆåŒ…å«RAGæ£€ç´¢ç»“æœï¼Œä¸ocr_card_rag_api.pyä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
        
        Args:
            base_prompt: åŸºç¡€æç¤ºè¯
            rag_results: RAGæ£€ç´¢ç»“æœ
            custom_prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
            
        Returns:
            å¢å¼ºåçš„å®Œæ•´æç¤ºè¯
        """
        if custom_prompt:
            prompt = custom_prompt
        else:
            prompt = base_prompt
            
        # å¦‚æœæœ‰RAGæ£€ç´¢ç»“æœï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if rag_results:
            rag_context = "\nåŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ï¼š\n"
            for rank, result in enumerate(rag_results, 1):
                filename = result["filename"]
                similarity = result["similarity"]
                rag_context += f"- å¡é¢{rank}: {filename} | ç›¸ä¼¼åº¦={similarity:.3f}\n"
            rag_context += "\n"
            filenames = [result["filename"].split(".")[0] for result in rag_results]
            banks = [filename.split("_")[0] for filename in filenames]
            prompt = rag_context + prompt
            prompt = prompt + (
                f"6. å¦‚æœæ˜¯é“¶è¡Œå¡ä¸”å­—æ®µåˆ—è¡¨åŒ…å«'å¡é¢ç±»å‹'ï¼Œåˆ™æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å¡«å……ï¼š\n"
                f"  - åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ç»“æœ{filenames}ï¼Œå¡«å……\"å¡é¢ç±»å‹\"å­—æ®µã€‚å­—æ®µå€¼è§„åˆ™å¦‚ä¸‹ï¼š\n"
                f"       -**ç¦æ­¢**è‡ªå®šä¹‰ã€ç”Ÿæˆã€çŒœæµ‹æˆ–ç¼–é€ æ–°çš„å¡é¢ç±»å‹å€¼ã€‚\n"
                f"       -å½“å‡ºç°ä»»ä½•ä¸ç¡®å®šã€æ¨¡ç³Šæˆ–ä¸åŒ¹é…æƒ…å†µæ—¶ï¼Œ\"å¡é¢ç±»å‹\"å­—æ®µçš„å€¼**å¿…é¡»ä¸”åªèƒ½ä¸º\"å…¶ä»–\"**ã€‚\n"
                f"       -è‹¥è¯†åˆ«å‡ºçš„\"å‘å¡è¡Œ\"å­—æ®µçš„å€¼å­˜åœ¨ä¸{banks}ä¸­é“¶è¡Œåç§°ç›¸åŒçš„æƒ…å†µï¼Œ"
                f"åˆ™\"å¡é¢ç±»å‹\"å­—æ®µçš„å€¼åªèƒ½ä»{filenames}ä¸­**ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ª**ã€‚\n"
            )
            
        return prompt

    def load_model(self, progress=gr.Progress()):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            return "âœ… æ¨¡å‹å·²ç»åŠ è½½å®Œæˆï¼", gr.update(interactive=True)

        if torch is None:
            return "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: æœªæ£€æµ‹åˆ°PyTorchï¼Œè¯·å…ˆå®‰è£…ã€‚", gr.update(interactive=False)

        try:
            progress(0.1, desc="æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
            if not os.path.exists(self.model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}", gr.update(interactive=False)

            progress(0.3, desc="åŠ è½½æ¨¡å‹...")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype="auto",
                device_map="cuda",
                load_in_4bit=False,
            )

            progress(0.7, desc="åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(self.model_path)

            progress(1.0, desc="å®Œæˆï¼")
            self.is_loaded = True

            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚", gr.update(interactive=True)

        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", gr.update(interactive=False)

    def _prepare_user_message(self, image, prompt):
        prompt_clean = (prompt or "").strip()
        resolved_image = image if image is not None else self.last_image
        if resolved_image is None:
            raise ValueError("âŒ è¯·ä¸Šä¼ å›¾åƒï¼")
        if not prompt_clean:
            raise ValueError("âŒ è¯·è¾“å…¥é—®é¢˜ï¼")
        if image is not None:
            self.last_image = image
        content = [
            {"type": "image", "image": resolved_image},
            {"type": "text", "text": prompt_clean},
        ]
        return prompt_clean, {"role": "user", "content": content}

    def _run_inference(self,
                       image,
                       prompt,
                       max_tokens,
                       temperature,
                       top_p,
                       top_k,
                       repetition_penalty,
                       prepared=None):
        if prepared is None:
            prompt_clean, user_message = self._prepare_user_message(image, prompt)
        else:
            prompt_clean, user_message = prepared
        messages = self.chat_messages + [user_message]

        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.model.device)

        generation_kwargs = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "do_sample": True if temperature > 0 else False,
            "repetition_penalty": repetition_penalty
        }

        start_time = time.time()
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, **generation_kwargs)
        generation_time = time.time() - start_time

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        response = output_text[0]

        assistant_message = {"role": "assistant", "content": [{"type": "text", "text": response}]}
        self.chat_messages.extend([user_message, assistant_message])
        return prompt_clean, response, generation_time

    def _clone_history(self, history):
        return [[turn[0], turn[1]] for turn in history]

    def _chunk_response(self, text, chunk_size=80):
        if not text:
            return []
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    def _parse_markdown_sections(self, markdown_text):
        """
        å°† Markdown æ–‡æœ¬æ‹†åˆ†ä¸º table/text æ®µï¼Œæ”¯æŒï¼š
        - ç®¡é“è¡¨æ ¼ï¼ˆ| a | b |ï¼‰
        - HTML <table>ï¼ˆè‹¥å­˜åœ¨ï¼‰
        å¹¶åœ¨è§£æå‰å¯¹å›´æ ä»£ç å—è¿›è¡Œå»å›´æ æ¸…æ´—ï¼Œç¡®ä¿å¯¼å‡ºä¸æ¸²æŸ“ä¸€è‡´ã€‚
        """
        sections = []
        if not markdown_text:
            return sections

        # 1) å…ˆå»æ‰å›´æ ï¼Œä½¿å¾—â€œä»£ç å—ä¸­çš„è¡¨æ ¼â€ä¹Ÿèƒ½è¢«è¯†åˆ«ä¸ºå¯å¯¼å‡ºçš„å†…å®¹
        cleaned_md = self._sanitize_markdown(markdown_text)

        # 2) å…ˆå°è¯•è§£æ HTML è¡¨æ ¼ï¼ˆè‹¥æ¨¡å‹è¾“å‡ºäº† <table>ï¼‰
        html_tables = []
        try:
            from bs4 import BeautifulSoup  # å¯é€‰ä¾èµ–
            soup = BeautifulSoup(cleaned_md, "html.parser")
            for t in soup.find_all("table"):
                headers = []
                header_row = t.find("tr")
                if header_row:
                    # å¦‚æœæœ‰ <th> ç”¨ thï¼›å¦åˆ™ç”¨é¦–è¡Œçš„ td ä½œä¸º header
                    ths = header_row.find_all("th")
                    if ths:
                        headers = [th.get_text(strip=True) for th in ths]
                        data_rows = header_row.find_next_siblings("tr")
                    else:
                        tds = header_row.find_all("td")
                        headers = [td.get_text(strip=True) for td in tds]
                        data_rows = header_row.find_next_siblings("tr")
                rows = []
                for r in (data_rows or []):
                    cols = r.find_all(["td", "th"])
                    rows.append([c.get_text(strip=True) for c in cols])
                if headers or rows:
                    html_tables.append({"type": "table", "header": headers, "rows": rows})
        except Exception:
            # å¦‚æœ bs4 ä¸åœ¨ç¯å¢ƒä¸­ï¼Œåˆ™ç•¥è¿‡ HTML è§£æ
            pass

        # 3) è§£æç®¡é“è¡¨æ ¼
        lines = cleaned_md.splitlines()
        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()

            # ç®¡é“è¡¨æ ¼åˆ¤å®šï¼šå½“å‰è¡Œå’Œä¸‹ä¸€è¡Œæ„æˆ header + åˆ†éš”
            is_table = (
                stripped.startswith("|")
                and stripped.count("|") >= 2
                and i + 1 < len(lines)
                and set(lines[i + 1].replace("|", "").strip()) <= set("-: ")
                and lines[i + 1].strip().startswith("|")
            )

            if is_table:
                header = [cell.strip() for cell in stripped.strip("|").split("|")]
                i += 2  # è·³è¿‡ header ä¸åˆ†éš”çº¿
                rows = []
                while i < len(lines):
                    row_line = lines[i].strip()
                    if not (row_line.startswith("|") and row_line.count("|") >= 2):
                        break
                    row = [cell.strip() for cell in row_line.strip("|").split("|")]
                    rows.append(row)
                    i += 1
                sections.append({"type": "table", "header": header, "rows": rows})
                continue

            # æ™®é€šæ–‡æœ¬å—ï¼ˆç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªè¡¨æ ¼æˆ–æ–‡ä»¶ç»“æŸï¼‰
            text_block = []
            while i < len(lines):
                current = lines[i]
                stripped_current = current.strip()
                next_is_table = (
                    stripped_current.startswith("|")
                    and stripped_current.count("|") >= 2
                    and i + 1 < len(lines)
                    and set(lines[i + 1].replace("|", "").strip()) <= set("-: ")
                    and lines[i + 1].strip().startswith("|")
                )
                if next_is_table:
                    break
                text_block.append(current)
                i += 1
                # ä¿ç•™ç©ºè¡Œï¼Œæ”¹å–„æ®µè½åˆ†éš”çš„å¯è¯»æ€§
                if i < len(lines) and lines[i] == "":
                    text_block.append(lines[i])

            text_content = "\n".join(text_block).strip("\n")
            if text_content:
                sections.append({"type": "text", "text": text_content})

        # 4) è‹¥å­˜åœ¨ HTML è¡¨ï¼Œä¼˜å…ˆæŠŠ HTML è¡¨ä¹ŸåŠ å…¥ï¼ˆæ”¾åœ¨è§£æç»“æœå‰é¢ï¼Œé¿å…é—æ¼ï¼‰
        if html_tables:
            # å°† HTML è¡¨æ’åœ¨æœ€å‰é¢ï¼ˆä¹Ÿå¯æ ¹æ®éœ€è¦åˆå¹¶/å»é‡ï¼‰
            sections = html_tables + sections

        return sections

    @staticmethod
    def _text_to_html_block(text: str) -> str:
        if not text:
            return ""
        escaped = html.escape(text)
        replaced = escaped.replace("\n", "<br>")
        return f'<div class="ocr-text">{replaced}</div>'

    @staticmethod
    def _table_to_html_block(header, rows) -> str:
        header = header or []
        rows = rows or []
        thead = ""
        if header:
            header_cells = "".join(f"<th>{html.escape(str(cell))}</th>" for cell in header)
            thead = f"<thead><tr>{header_cells}</tr></thead>"
        body_rows = []
        for row in rows:
            row_cells = "".join(f"<td>{html.escape(str(cell))}</td>" for cell in row)
            body_rows.append(f"<tr>{row_cells}</tr>")
        tbody = f"<tbody>{''.join(body_rows)}</tbody>" if body_rows else "<tbody></tbody>"
        return f'<table class="ocr-table">{thead}{tbody}</table>'

    def _render_sections_as_html(self, markdown_text: str) -> str:
        if not markdown_text:
            return ""
        sections = self._parse_markdown_sections(markdown_text)
        if not sections:
            escaped = html.escape(markdown_text.strip())
            return f"<pre>{escaped}</pre>" if escaped else ""
        blocks = []
        for section in sections:
            if section.get("type") == "table":
                blocks.append(self._table_to_html_block(section.get("header"), section.get("rows")))
            elif section.get("type") == "text":
                blocks.append(self._text_to_html_block(section.get("text", "")))
        return '<div class="ocr-preview">' + "".join(blocks) + "</div>"

    def chat_with_image(self, image, text, history, max_tokens, temperature, top_p, top_k, repetition_penalty: float = 1.0, presence_penalty: float = 1.5):
        """ä¸å›¾åƒå¯¹è¯ï¼ˆæµå¼åé¦ˆï¼‰"""
        original_text = text

        if not self.is_loaded:
            yield history, original_text, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
            return

        try:
            prepared = self._prepare_user_message(image, text)
        except ValueError as exc:
            yield history, original_text, str(exc)
            return

        prompt_clean, _ = prepared
        history_copy = self._clone_history(history)
        history_copy.append([f"ğŸ‘¤ {prompt_clean}", "ğŸ¤– æ­£åœ¨æ€è€ƒ..."])
        yield self._clone_history(history_copy), original_text, "ğŸ¤– æ­£åœ¨æ€è€ƒ..."

        try:
            _, response, generation_time = self._run_inference(
                image,
                text,
                max_tokens,
                temperature,
                top_p,
                top_k,
                repetition_penalty,
                prepared=prepared
            )
        except Exception as e:
            history_copy[-1][1] = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            self.chat_history = self._clone_history(history_copy)
            yield self._clone_history(history_copy), original_text, f"âŒ é”™è¯¯: {str(e)}"
            return

        assembled = ""
        chunks = self._chunk_response(response)
        if not chunks:
            chunks = [""]
        for chunk in chunks:
            assembled += chunk
            history_copy[-1][1] = f"ğŸ¤– {assembled}â–Œ"
            yield self._clone_history(history_copy), original_text, f"ğŸ¤– {assembled}â–Œ"

        stats = (
            f"â±ï¸ ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’ | ğŸ“ ç”Ÿæˆé•¿åº¦: {len(response)}å­—ç¬¦"
            f" | âš™ï¸ æœ€å¤§é•¿åº¦: {max_tokens}"
        )
        if max_tokens > 1024:
            stats += " | â³ æç¤º: è¾ƒå¤§çš„æœ€å¤§é•¿åº¦å¯èƒ½å»¶é•¿ç”Ÿæˆæ—¶é—´"
        history_copy[-1][1] = f"ğŸ¤– {response}"
        self.chat_history = self._clone_history(history_copy)
        yield self._clone_history(history_copy), original_text, stats

    def _sanitize_markdown(self, text: str) -> str:
        if not text:
            return ""
        s = text.strip()
        lines = s.splitlines()
        out = []
        in_fence = False
        for line in lines:
            stripped = line.strip()
            if stripped.startswith("```"):
                in_fence = not in_fence
                continue
            if not in_fence:
                out.append(line)
        cleaned = "\n".join(out).strip()
        return cleaned if cleaned else s

    def ocr_analysis(self, image, prompt: str = None):
        """OCRæ–‡å­—è¯†åˆ«ï¼Œå¯é€‰è‡ªå®šä¹‰æç¤ºè¯"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        default_prompt = (
            "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œå°½é‡è¿˜åŸåŸæœ¬æ ·å¼ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚"
            " è¯·ç¡®ä¿æ‰€æœ‰å¸¦æ ·å¼æˆ–è¡¨æ ¼å†…å®¹ä½¿ç”¨Markdownè¡¨æ ¼è¡¨ç¤ºã€‚"
        )
        effective_prompt = (prompt or "").strip() or default_prompt
        try:
            prompt_clean, response, _ = self._run_inference(
                image,
                effective_prompt,
                max_tokens=1024,
                temperature=0.7,
                top_p=0.8,
                top_k=50,
                repetition_penalty=1.0
            )
            cleaned = self._sanitize_markdown(response)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {cleaned}"])
            self.last_ocr_markdown = f"## OCRè¯†åˆ«ç»“æœ\n\n{cleaned}"
            self.last_ocr_html = "<h2>OCRè¯†åˆ«ç»“æœ</h2>" + self._render_sections_as_html(cleaned)
            return f"ğŸ“ OCRè¯†åˆ«ç»“æœ:\n\n{cleaned}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ OCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def ocr_card(self, image, prompt: str = None):
        """å¡è¯OCRè¯†åˆ«ï¼šèº«ä»½è¯/é“¶è¡Œå¡/é©¾é©¶è¯ç­‰ç»“æ„åŒ–æå–ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œæµç¨‹ä¸APIç‰ˆæœ¬ä¸€è‡´ï¼‰"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        # ä½¿ç”¨ä¸ocr_card_apiç›¸åŒçš„é»˜è®¤æç¤ºè¯
        default_prompt = (
            "ä½ æ˜¯ä¸“ä¸šçš„å¡è¯OCRå¼•æ“ï¼Œè¯·å¯¹è¾“å…¥å›¾ç‰‡è¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œå¹¶ä»…è¾“å‡ºMarkdownè¡¨æ ¼ã€‚\n"
            "\n"
            "ä»»åŠ¡è¦æ±‚å¦‚ä¸‹ï¼š\n"
            "\n"
            "1. è¯†åˆ«å¡è¯ç±»å‹ï¼šåªå…è®¸ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©ä¸€ç§ï¼š\n"
            "   - èº«ä»½è¯ / é“¶è¡Œå¡ / é©¾é©¶è¯ / æŠ¤ç…§ / å·¥ç‰Œ / å…¶ä»–ã€‚\n"
            "   Markdownè¡¨æ ¼ä¸­æ·»åŠ \"å¡è¯ç±»å‹\"å­—æ®µï¼Œå¹¶ç”¨ç±»åˆ«é€‰æ‹©èµ‹å€¼ã€‚\n"
            "   **é‡è¦**ï¼šå¦‚æœè¯†åˆ«ä¸ºé“¶è¡Œå¡ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆç¬¬3æ¡é“¶è¡Œå¡ç‰¹æ®Šè¦æ±‚ï¼\n"
            "\n"
            "2. è¾“å‡ºæ ¼å¼ï¼š\n"
            "   - ä»¥Markdownè¡¨æ ¼å½¢å¼è¾“å‡ºæ‰€æœ‰è¯†åˆ«å‡ºçš„å…³é”®å­—æ®µåŠå…¶å¯¹åº”çš„å€¼ã€‚\n"
            "   - è‹¥å­—æ®µä¸­åŒ…å«\"å¡å·\"ï¼Œè¯·ç¡®ä¿è¯¥å­—æ®µçš„å€¼ä»…åŒ…å«æ•°å­—ã€‚\n"
            "   - ä¸è¦ä½¿ç”¨ä»£ç å—æ ‡è®°ç¬¦å·ï¼ˆä¾‹å¦‚ ``` ï¼‰ã€‚\n"
            "\n"
            "3. é“¶è¡Œå¡ç‰¹æ®Šè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\n"
            "   å¦‚æœè¯†åˆ«çš„å¡è¯ç±»å‹æ˜¯é“¶è¡Œå¡ï¼Œå¿…é¡»åœ¨Markdownè¡¨æ ¼çš„æœ€åé¢å¤–æ·»åŠ ä¸€ä¸ªå­—æ®µï¼š\n"
            "   - å­—æ®µåï¼šå¡é¢ç±»å‹ï¼ˆå¿…é¡»æ·»åŠ ï¼Œä¸å¯çœç•¥ï¼‰ã€‚\n"
            "   - åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ç»“æœï¼Œå¡«å……\"å¡é¢ç±»å‹\"å­—æ®µã€‚å­—æ®µå€¼è§„åˆ™å¦‚ä¸‹ï¼š\n"
            "       â‘  å½“å‡ºç°ä»»ä½•ä¸ç¡®å®šã€æ¨¡ç³Šæˆ–ä¸åŒ¹é…æƒ…å†µæ—¶ï¼Œ\"å¡é¢ç±»å‹\"å­—æ®µçš„å€¼**å¿…é¡»ä¸”åªèƒ½ä¸º\"å…¶ä»–\"**ï¼Œä¸å¾—å¡«å†™ç›¸ä¼¼å›¾ç‰‡åæˆ–å…¶ä»–æ–‡æœ¬ã€‚\n"
            "       â‘¡ è‹¥è¯†åˆ«å‡ºçš„\"å‘å¡è¡Œ\"å­—æ®µçš„å€¼ä¸è¿™äº›ç›¸ä¼¼å¡è¯æ–‡ä»¶åä¸­`_`å‰é¢çš„é“¶è¡Œåç§°ç›¸åŒï¼Œ"
            "åˆ™\"å¡é¢ç±»å‹\"å­—æ®µçš„å€¼åªèƒ½ä»ç›¸ä¼¼å¡è¯æ–‡ä»¶åä¸­**ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ª**ï¼Œæ ¼å¼ä¸º`é“¶è¡Œåç§°_å¡é¢ç±»å‹`ï¼Œå»æ‰æ–‡ä»¶åç¼€åï¼Œå¦‚`ä¸­å›½é“¶è¡Œ_visaå¡`ã€‚\n"
            "       â‘¢ ç¦æ­¢è‡ªå®šä¹‰ã€ç”Ÿæˆã€çŒœæµ‹æˆ–ç¼–é€ æ–°çš„å¡é¢ç±»å‹å€¼ã€‚ä»»ä½•ä¸å­˜åœ¨åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯æ–‡ä»¶åçš„å€¼éƒ½è§†ä¸ºé”™è¯¯ã€‚\n"
            "   **é‡è¦æé†’**ï¼šé“¶è¡Œå¡çš„Markdownè¡¨æ ¼å¿…é¡»åŒ…å«\"å¡é¢ç±»å‹\"å­—æ®µï¼Œè¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¸èƒ½çœç•¥ï¼\n"
            "   - å¦‚æœä¸æ˜¯é“¶è¡Œå¡ï¼Œåˆ™ä¸æ·»åŠ \"å¡é¢ç±»å‹\"å­—æ®µã€‚\n"
            "\n"
            "4. è¾“å‡ºé™åˆ¶ï¼š\n"
            "   - æœ€ç»ˆè¾“å‡ºåªåŒ…å«Markdownè¡¨æ ¼ã€‚\n"
            "   - ç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Šæ€§å†…å®¹ã€‚\n"
            "   - å¦‚æœæ˜¯é“¶è¡Œå¡ï¼Œè¡¨æ ¼ä¸­å¿…é¡»åŒ…å«\"å¡é¢ç±»å‹\"å­—æ®µï¼Œå¦åˆ™è¾“å‡ºä¸å®Œæ•´ã€‚\n"
        )

        effective_prompt = (prompt or "").strip() or default_prompt

        # RAGæ£€ç´¢ï¼ˆä½¿ç”¨ä¸APIç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
        rag_results = []
        try:
            self._ensure_card_rag_loaded()
            if self.card_rag_store and getattr(self.card_rag_store, "image_embeddings", None):
                rag_results = self._rag_search_card(image, top_k=3)
        except Exception as e:
            print(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥: {str(e)}")
            rag_results = []

        # åœ¨ç»ˆç«¯è¾“å‡ºRAGç›¸ä¼¼åº¦åŒ¹é…ç»“æœï¼ˆä¸APIç‰ˆæœ¬ä¸€è‡´ï¼‰
        if rag_results:
            print("\n" + "=" * 60)
            print("ğŸ“Š RAGç›¸ä¼¼åº¦åŒ¹é…ç»“æœ")
            print("=" * 60)
            print(f"æ‰¾åˆ° {len(rag_results)} å¼ ç›¸ä¼¼å›¾ç‰‡ï¼š\n")
            for i, r in enumerate(rag_results, 1):
                filename = r.get("filename", "æœªçŸ¥")
                similarity = r.get("similarity", 0.0)
                print(f"  {i}. {filename}")
                print(f"     ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")
            print("=" * 60 + "\n")
        else:
            print("\nâš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡\n")

        # æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆä½¿ç”¨ä¸APIç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
        enhanced_prompt = self._build_enhanced_prompt_card(
            base_prompt=default_prompt,
            rag_results=rag_results,
            custom_prompt=effective_prompt if (prompt or "").strip() else None
        )

        # åœ¨ç»ˆç«¯è¾“å‡ºå‘é€ç»™æ¨¡å‹çš„å®Œæ•´promptï¼ˆä¸APIç‰ˆæœ¬ä¸€è‡´ï¼‰
        print("\n" + "=" * 80)
        print("ğŸ“ å‘é€ç»™æ¨¡å‹çš„å®Œæ•´Prompt")
        print("=" * 80)
        print(enhanced_prompt)
        print("=" * 80 + "\n")

        try:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¨ç†
            prompt_clean, response, _ = self._run_inference(
                image,
                enhanced_prompt,
                max_tokens=1024,
                temperature=0.3,
                top_p=0.8,
                top_k=40,
                repetition_penalty=1.05
            )
            cleaned = self._sanitize_markdown(response)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {cleaned}"])
            self.last_ocr_markdown = f"## å¡è¯OCRè¯†åˆ«ç»“æœ\n\n{cleaned}"
            self.last_ocr_html = "<h2>å¡è¯OCRè¯†åˆ«ç»“æœ</h2>" + self._render_sections_as_html(cleaned)
            return f"ğŸªª å¡è¯OCRè¯†åˆ«ç»“æœ:\n\n{cleaned}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ å¡è¯OCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def ocr_card_api(self, image, prompt: str = None):
        """å¡è¯OCRè¯†åˆ«ï¼ˆAPIè°ƒç”¨ + RAGå¢å¼ºï¼‰"""
        # æ³¨ï¼šå¦‚æ— éœ€å¼ºåˆ¶æœ¬åœ°æ¨¡å‹åŠ è½½ï¼Œå¯ç§»é™¤æ­¤åˆ¤æ–­
        try:
            self._ensure_card_api_loaded()
            if self.card_api is None:
                return "ï¿½?å¡è¯OCR APIåˆå§‹åŒ–å¤±è´¥"
            default_prompt = (
                "ä½ æ˜¯ä¸“ä¸šçš„å¡è¯OCRå¼•æ“ï¼Œè¯·å¯¹è¾“å…¥å›¾ç‰‡è¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œå¹¶ä»…è¾“å‡ºMarkdownè¡¨æ ¼ã€‚\n"
                "\n"
                "ä»»åŠ¡è¦æ±‚å¦‚ä¸‹ï¼š\n"
                "\n"
            "1. è¯†åˆ«å¡è¯ç±»å‹ï¼šåªå…è®¸ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©ä¸€ç§ï¼š\n"
            "   - èº«ä»½è¯ / é“¶è¡Œå¡ / é©¾é©¶è¯ / æŠ¤ç…§ / å·¥ç‰Œ / å…¶ä»–ã€‚\n"
            "   Markdownè¡¨æ ¼ä¸­æ·»åŠ â€œå¡è¯ç±»å‹â€å­—æ®µï¼Œå¹¶ç”¨ç±»åˆ«é€‰æ‹©èµ‹å€¼ã€‚\n"
            "   **é‡è¦**ï¼šå¦‚æœè¯†åˆ«ä¸ºé“¶è¡Œå¡ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆç¬¬3æ¡é“¶è¡Œå¡ç‰¹æ®Šè¦æ±‚ï¼\n"
            "\n"
            "2. è¾“å‡ºæ ¼å¼ï¼š\n"
            "   - ä»¥Markdownè¡¨æ ¼å½¢å¼è¾“å‡ºæ‰€æœ‰è¯†åˆ«å‡ºçš„å…³é”®å­—æ®µåŠå…¶å¯¹åº”çš„å€¼ã€‚\n"
            "   - è‹¥å­—æ®µä¸­åŒ…å«â€œå¡å·â€ï¼Œè¯·ç¡®ä¿è¯¥å­—æ®µçš„å€¼ä»…åŒ…å«æ•°å­—ã€‚\n"
            "   - ä¸è¦ä½¿ç”¨ä»£ç å—æ ‡è®°ç¬¦å·ï¼ˆä¾‹å¦‚ ``` ï¼‰ã€‚\n"
            "\n"
            "3. é“¶è¡Œå¡ç‰¹æ®Šè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\n"
            "   å¦‚æœè¯†åˆ«çš„å¡è¯ç±»å‹æ˜¯é“¶è¡Œå¡ï¼Œå¿…é¡»åœ¨Markdownè¡¨æ ¼çš„æœ€åé¢å¤–æ·»åŠ ä¸€ä¸ªå­—æ®µï¼š\n"
            "   - å­—æ®µåï¼šå¡é¢ç±»å‹ï¼ˆå¿…é¡»æ·»åŠ ï¼Œä¸å¯çœç•¥ï¼‰ã€‚\n"
            "   - åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ç»“æœï¼Œå¡«å……â€œå¡é¢ç±»å‹â€å­—æ®µã€‚å­—æ®µå€¼è§„åˆ™å¦‚ä¸‹ï¼š\n"
            "       â‘  å½“å‡ºç°ä»»ä½•ä¸ç¡®å®šã€æ¨¡ç³Šæˆ–ä¸åŒ¹é…æƒ…å†µæ—¶ï¼Œâ€œå¡é¢ç±»å‹â€å­—æ®µçš„å€¼**å¿…é¡»ä¸”åªèƒ½ä¸ºâ€œå…¶ä»–â€**ï¼Œä¸å¾—å¡«å†™ç›¸ä¼¼å›¾ç‰‡åæˆ–å…¶ä»–æ–‡æœ¬ã€‚\n"
            "       â‘¡ è‹¥è¯†åˆ«å‡ºçš„â€œå‘å¡è¡Œâ€å­—æ®µçš„å€¼ä¸è¿™äº›ç›¸ä¼¼å¡è¯æ–‡ä»¶åä¸­`_`å‰é¢çš„é“¶è¡Œåç§°ç›¸åŒï¼Œ"
            "åˆ™â€œå¡é¢ç±»å‹â€å­—æ®µçš„å€¼åªèƒ½ä»ç›¸ä¼¼å¡è¯æ–‡ä»¶åä¸­**ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ª**ï¼Œæ ¼å¼ä¸º`é“¶è¡Œåç§°_å¡é¢ç±»å‹`ï¼Œå»æ‰æ–‡ä»¶åç¼€åï¼Œå¦‚`ä¸­å›½é“¶è¡Œ_visaå¡`ã€‚\n"
            "       â‘¢ ç¦æ­¢è‡ªå®šä¹‰ã€ç”Ÿæˆã€çŒœæµ‹æˆ–ç¼–é€ æ–°çš„å¡é¢ç±»å‹å€¼ã€‚ä»»ä½•ä¸å­˜åœ¨åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯æ–‡ä»¶åçš„å€¼éƒ½è§†ä¸ºé”™è¯¯ã€‚\n"
            "   **é‡è¦æé†’**ï¼šé“¶è¡Œå¡çš„Markdownè¡¨æ ¼å¿…é¡»åŒ…å«â€œå¡é¢ç±»å‹â€å­—æ®µï¼Œè¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¸èƒ½çœç•¥ï¼\n"
            "   - å¦‚æœä¸æ˜¯é“¶è¡Œå¡ï¼Œåˆ™ä¸æ·»åŠ â€œå¡é¢ç±»å‹â€å­—æ®µã€‚\n"
            "\n"
            "4. è¾“å‡ºé™åˆ¶ï¼š\n"
            "   - æœ€ç»ˆè¾“å‡ºåªåŒ…å«Markdownè¡¨æ ¼ã€‚\n"
            "   - ç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Šæ€§å†…å®¹ã€‚\n"
            "   - å¦‚æœæ˜¯é“¶è¡Œå¡ï¼Œè¡¨æ ¼ä¸­å¿…é¡»åŒ…å«â€œå¡é¢ç±»å‹â€å­—æ®µï¼Œå¦åˆ™è¾“å‡ºä¸å®Œæ•´ã€‚\n"
            )

            effective_prompt = (prompt or "").strip() or default_prompt
            result = self.card_api.recognize_card(
                image,
                custom_prompt=effective_prompt,
                use_rag=True,
            )
            if not result.get("success"):
                return f"ï¿½?å¡è¯OCR APIè°ƒç”¨å¤±è´¥: {result.get('error') or 'æœªçŸ¥é”™è¯¯'}"

            # åœ¨ç»ˆç«¯è¾“å‡ºRAGç›¸ä¼¼åº¦åŒ¹é…ç»“æœ
            rag_info = result.get("rag_info")
            if rag_info and rag_info.get("enabled") and rag_info.get("results"):
                print("\n" + "=" * 60)
                print("ğŸ“Š RAGç›¸ä¼¼åº¦åŒ¹é…ç»“æœ")
                print("=" * 60)
                print(f"æ‰¾åˆ° {len(rag_info['results'])} å¼ ç›¸ä¼¼å›¾ç‰‡ï¼š\n")
                for i, r in enumerate(rag_info["results"], 1):
                    filename = r.get("filename", "æœªçŸ¥")
                    similarity = r.get("similarity", 0.0)
                    print(f"  {i}. {filename}")
                    print(f"     ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")
                print("=" * 60 + "\n")
            elif rag_info and not rag_info.get("enabled"):
                print(f"\nâš ï¸ RAGæœªå¯ç”¨: {rag_info.get('reason', 'æœªçŸ¥åŸå› ')}\n")
            else:
                print("\nâš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡\n")

            cleaned = self._sanitize_markdown(result.get("result") or "")
            self.last_ocr_markdown = f"## å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰ç»“æœ\n\n{cleaned}"
            self.last_ocr_html = "<h2>å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰ç»“æœ</h2>" + self._render_sections_as_html(cleaned)
            return f"ğŸªª å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰ç»“æœ:\n\n{cleaned}"
        except Exception as e:
            return f"ï¿½?å¡è¯OCR APIè¯†åˆ«å¤±è´¥: {str(e)}"

    def ocr_receipt(self, image, prompt: str = None):
        """ç¥¨æ®OCRè¯†åˆ«ï¼šå‘ç¥¨/å°ç¥¨ç­‰è¡¨æ ¼ä¸å…³é”®é¡¹è§£æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        default_prompt = (
            "ä½ æ˜¯å‘ç¥¨/å°ç¥¨OCRä¸“å®¶ã€‚è¯·è§£æå›¾ç‰‡ä¸­çš„ç¥¨æ®å¹¶è¾“å‡ºï¼š\n"
            "- ä»¥Markdownè¡¨æ ¼ç»™å‡ºå…³é”®ä¿¡æ¯ï¼šç¥¨æ®ç±»å‹ã€å¼€ç¥¨æ—¥æœŸã€å‘ç¥¨ä»£ç ã€å‘ç¥¨å·ç ã€æ ¡éªŒç ã€è´­ä¹°æ–¹ã€é”€å”®æ–¹ã€ç¨å·ã€é¡¹ç›®ã€æ•°é‡ã€å•ä»·ã€é‡‘é¢ã€ç¨ç‡ã€ç¨é¢ã€åˆè®¡é‡‘é¢(å«ç¨/ä¸å«ç¨)ï¼›\n"
            "- è‹¥æ£€æµ‹åˆ°å¤šè¡Œé¡¹ç›®ï¼Œè¯·ä»¥è¡¨æ ¼å½¢å¼é€è¡Œåˆ—å‡ºï¼›\n"
            "- è¡¨æ ¼ä¸‹æ–¹ç»™å‡ºè¯†åˆ«ç½®ä¿¡åº¦ä¸å¯ç–‘é¡¹æç¤ºï¼›\n"
            "- ä¸è¦ä½¿ç”¨å›´æ ä»£ç å—ï¼Œä¿æŒMarkdownå¯æ¸²æŸ“ã€‚"
        )
        effective_prompt = (prompt or "").strip() or default_prompt
        try:
            prompt_clean, response, _ = self._run_inference(
                image,
                effective_prompt,
                max_tokens=1536,
                temperature=0.2,
                top_p=0.8,
                top_k=40,
                repetition_penalty=1.05
            )
            cleaned = self._sanitize_markdown(response)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {cleaned}"])
            self.last_ocr_markdown = f"## ç¥¨æ®OCRè¯†åˆ«ç»“æœ\n\n{cleaned}"
            self.last_ocr_html = "<h2>ç¥¨æ®OCRè¯†åˆ«ç»“æœ</h2>" + self._render_sections_as_html(cleaned)
            return f"ğŸ§¾ ç¥¨æ®OCRè¯†åˆ«ç»“æœ:\n\n{cleaned}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ ç¥¨æ®OCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def ocr_agreement(self, image, prompt: str = None):
        """åè®®OCRè¯†åˆ«ï¼šåˆåŒ/åè®®æ®µè½ä¸æ¡æ¬¾è§£æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        default_prompt = (
            "ä½ æ˜¯åˆåŒ/åè®®OCRä¸æ¡æ¬¾è§£æåŠ©æ‰‹ã€‚è¯·å®Œæˆï¼š\n"
            "1) è¯†åˆ«å…¨æ–‡ï¼Œä¿æŒæ®µè½ç»“æ„ï¼›\n"
            "2) ä»¥Markdownè¡¨æ ¼æç‚¼å…³é”®ä¿¡æ¯ï¼šåˆåŒåç§°ã€ç”²æ–¹ã€ä¹™æ–¹ã€ç­¾ç½²æ—¥æœŸã€ç”Ÿæ•ˆæ—¥æœŸã€ç»ˆæ­¢æ—¥æœŸã€é‡‘é¢/å¸ç§ã€è¿çº¦æ¡æ¬¾ã€äº‰è®®è§£å†³ã€ç­¾ç« æƒ…å†µï¼›\n"
            "3) å¦‚æœ‰ç¼–å·çš„æ¡æ¬¾ï¼Œä¿ç•™ç¼–å·å¹¶é€æ¡åˆ—å‡ºï¼›\n"
            "4) åœ¨æœ«å°¾ç»™å‡ºâ€œé£é™©æç¤ºâ€åˆ—è¡¨ï¼ˆå¦‚ç©ºç™½å¤„ã€æ¶‚æ”¹å¤„ã€å…³é”®è¦ç´ ç¼ºå¤±ç­‰ï¼‰ï¼›\n"
            "5) ä¸è¦è¾“å‡ºå›´æ ä»£ç å—ã€‚"
        )
        effective_prompt = (prompt or "").strip() or default_prompt
        try:
            prompt_clean, response, _ = self._run_inference(
                image,
                effective_prompt,
                max_tokens=2048,
                temperature=0.3,
                top_p=0.8,
                top_k=40,
                repetition_penalty=1.05
            )
            cleaned = self._sanitize_markdown(response)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {cleaned}"])
            self.last_ocr_markdown = f"## åè®®OCRè¯†åˆ«ç»“æœ\n\n{cleaned}"
            self.last_ocr_html = "<h2>åè®®OCRè¯†åˆ«ç»“æœ</h2>" + self._render_sections_as_html(cleaned)
            return f"ğŸ“„ åè®®OCRè¯†åˆ«ç»“æœ:\n\n{cleaned}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ åè®®OCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def spatial_analysis(self, image, prompt: str = None):
        """ç©ºé—´æ„ŸçŸ¥åˆ†æï¼Œå¯é€‰è‡ªå®šä¹‰æç¤ºè¯"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        default_prompt = (
            "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»ï¼ŒåŒ…æ‹¬ç›¸å¯¹ä½ç½®ã€è§†è§’ã€é®æŒ¡ã€æ·±åº¦ä¸è·ç¦»æ„Ÿï¼Œå¹¶ç»™å‡ºæ•´ä½“å¸ƒå±€æè¿°ã€‚"
        )
        effective_prompt = (prompt or "").strip() or default_prompt
        try:
            prompt_clean, response, _ = self._run_inference(
                image,
                effective_prompt,
                max_tokens=768,
                temperature=0.7,
                top_p=0.8,
                top_k=50,
                repetition_penalty=1.0
            )
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {response}"])
            return f"ğŸ“ ç©ºé—´åˆ†æç»“æœ:\n\n{response}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ ç©ºé—´åˆ†æå¤±è´¥: {str(e)}"

    def visual_coding(self, image, output_format: str = "HTML", prompt: str = None):
        """è§†è§‰ç¼–ç¨‹ç”Ÿæˆä»£ç ï¼Œå¯é€‰è‡ªå®šä¹‰æç¤ºè¯"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        base_prompts = {
            "HTML": "è¯·æ ¹æ®å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„HTMLç»“æ„ä»£ç ï¼ŒåŒ…å«å¿…è¦çš„è¯­ä¹‰æ ‡ç­¾ã€‚",
            "CSS": "è¯·ä¸ºè¯¥å›¾ç‰‡å¯¹åº”çš„ç•Œé¢ç”Ÿæˆåˆç†çš„CSSæ ·å¼ä»£ç ï¼ŒåŒ…æ‹¬å¸ƒå±€ä¸é¢œè‰²ã€‚",
            "JavaScript": "è¯·æ ¹æ®å›¾ç‰‡äº¤äº’ç”ŸæˆJavaScriptä»£ç ç¤ºä¾‹ï¼ŒåŒ…å«å¿…è¦çš„äº‹ä»¶ä¸é€»è¾‘ã€‚",
            "Python": "è¯·ç”Ÿæˆèƒ½å¤ç°è¯¥ç•Œé¢/å¸ƒå±€çš„Pythonç¤ºä¾‹ä»£ç ï¼ˆå¦‚ä½¿ç”¨streamlitæˆ–flaskçš„ä¼ªä»£ç ï¼‰ã€‚",
        }
        default_prompt = base_prompts.get(output_format, base_prompts["HTML"]) + " è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚"
        effective_prompt = (prompt or "").strip() or default_prompt
        try:
            prompt_clean, response, _ = self._run_inference(
                image,
                effective_prompt,
                max_tokens=1024,
                temperature=0.4,
                top_p=0.8,
                top_k=50,
                repetition_penalty=1.0
            )
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {response}"])
            return response
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ è§†è§‰ç¼–ç¨‹å¤±è´¥: {str(e)}"

    def batch_analysis(self, images, analysis_type):
        """æ‰¹é‡åˆ†æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"

        if not images:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"

        results = []

        for i, image in enumerate(images):
            try:
                if analysis_type == "æè¿°":
                    prompt = "è¯·çœŸå®ã€è¯¦ç»†ã€å®¢è§‚åœ°æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
                elif analysis_type == "OCR":
                    prompt = "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œå°½é‡è¿˜åŸåŸæœ¬æ ·å¼ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚"
                elif analysis_type == "ç©ºé—´åˆ†æ":
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»å’Œç‰©ä½“ä½ç½®ï¼ŒåŒ…æ‹¬ç›¸å¯¹ä½ç½®ã€è§†è§’ã€é®æŒ¡ã€æ·±åº¦ä¸è·ç¦»æ„Ÿï¼Œå¹¶ç»™å‡ºæ•´ä½“å¸ƒå±€æè¿°ã€‚"
                elif analysis_type == "æƒ…æ„Ÿåˆ†æ":
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæˆ–æ°›å›´ã€‚"
                else:
                    prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ã€‚"

                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": prompt},
                        ],
                    }
                ]

                inputs = self.processor.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt"
                )
                inputs = inputs.to(self.model.device)

                with torch.no_grad():
                    generated_ids = self.model.generate(**inputs, max_new_tokens=512)

                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = self.processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                results.append(f"ğŸ“· å›¾åƒ {i+1}:\n{output_text[0]}\n" + "="*50 + "\n")

            except Exception as e:
                results.append(f"ğŸ“· å›¾åƒ {i+1}: âŒ åˆ†æå¤±è´¥ - {str(e)}\n" + "="*50 + "\n")

        return "".join(results)

    def compare_images(self, image1, image2, comparison_type):
        """å›¾åƒå¯¹æ¯”"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"

        if image1 is None or image2 is None:
            return "âŒ è¯·ä¸Šä¼ ä¸¤å¼ å›¾åƒè¿›è¡Œå¯¹æ¯”ï¼"

        try:
            if comparison_type == "ç›¸ä¼¼æ€§":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œåˆ†æå®ƒä»¬çš„ç›¸ä¼¼ä¹‹å¤„å’Œä¸åŒä¹‹å¤„ã€‚"
            elif comparison_type == "é£æ ¼":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡çš„è‰ºæœ¯é£æ ¼ã€è‰²å½©æ­é…å’Œæ„å›¾ç‰¹ç‚¹ã€‚"
            elif comparison_type == "å†…å®¹":
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œåˆ†æå®ƒä»¬æè¿°çš„åœºæ™¯æˆ–ä¸»é¢˜ã€‚"
            else:
                prompt = "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œæä¾›è¯¦ç»†çš„å¯¹æ¯”åˆ†æã€‚"

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image1},
                        {"type": "image", "image": image2},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]

            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)

            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024)

            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )

            return f"ğŸ” å¯¹æ¯”åˆ†æç»“æœ:\n\n{output_text[0]}"

        except Exception as e:
            return f"âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}"

    def export_chat_history(self):
        """å¯¼å‡ºå¯¹è¯å†å²"""
        if not self.chat_history:
            return "âŒ æ²¡æœ‰å¯¹è¯å†å²å¯å¯¼å‡ºï¼"

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_history_{timestamp}.json"

            # ä¿å­˜ä¸ºJSONæ ¼å¼
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.chat_history, f, ensure_ascii=False, indent=2)

            return f"âœ… å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°: {filename}"

        except Exception as e:
            return f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}"

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
        self.chat_messages = []
        self.last_image = None
        self.last_saved_image_path = None
        self.last_image_digest = None
        self.last_ocr_markdown = None
        self.last_ocr_html = None
        if hasattr(self, "session_turn_image_paths"):
            self.session_turn_image_paths.clear()
        return []

    def export_last_ocr(self):
        if not self.last_ocr_markdown:
            return "âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ–‡æœ¬æ ·å¼ï¼Œè¯·å…ˆæ‰§è¡Œä¸€æ¬¡OCRè¯†åˆ«ï¼"

        export_dir = os.path.join("ocr_exports")
        os.makedirs(export_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        sections = self._parse_markdown_sections(self.last_ocr_markdown)

        excel_path = os.path.join(export_dir, f"ocr_{timestamp}.xlsx")
        excel_note = ""
        try:
            from openpyxl import Workbook

            wb = Workbook()
            ws = wb.active
            ws.title = "è¡¨æ ¼1" if sections else "OCRæ–‡æœ¬"
            table_idx = 0
            for section in sections:
                if section["type"] == "table":
                    table_idx += 1
                    if table_idx > 1:
                        ws = wb.create_sheet(title=f"è¡¨æ ¼{table_idx}")
                    ws.append(section["header"])
                    for row in section["rows"]:
                        ws.append(row)
                elif section["type"] == "text" and section["text"]:
                    if table_idx > 0:
                        ws = wb.create_sheet(title=f"æ–‡æœ¬{table_idx}")
                    for line in section["text"].splitlines():
                        ws.append([line])
            if not sections:
                for line in self.last_ocr_markdown.splitlines():
                    ws.append([line])
            wb.save(excel_path)
        except Exception as exc:
            excel_path = os.path.join(export_dir, f"ocr_{timestamp}.csv")
            with open(excel_path, "w", encoding="utf-8", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["OCR Result"])
                for line in self.last_ocr_markdown.splitlines():
                    writer.writerow([line])
            excel_note = f"âš ï¸ Excelå¯¼å‡ºå¤±è´¥({exc})ï¼Œå·²ä¿å­˜ä¸ºCSV"

        json_path = os.path.join(export_dir, f"ocr_{timestamp}.json")
        json_content = {
            "markdown": self.last_ocr_markdown,
            "sections": sections,
        }
        with open(json_path, "w", encoding='utf-8') as f:
            json.dump(json_content, f, ensure_ascii=False, indent=2)

        message_lines = [
            "âœ… æ–‡æœ¬æ ·å¼å·²ä¿å­˜ï¼š",
            f"- Excel: {excel_path}" + (f" ({excel_note})" if excel_note else ""),
            f"- JSON: {json_path}",
        ]
        return "\n".join(message_lines)


DEFAULT_TASK_PROMPTS = {
    "ä»»åŠ¡é—®ç­”": "è¯·æ ¹æ®å›¾ç‰‡å®ŒæˆæŒ‡å®šä»»åŠ¡ã€‚",
    "OCRè¯†åˆ«": "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¸¦æ ·å¼æˆ–è¡¨æ ¼å†…å®¹ä½¿ç”¨Markdownè¡¨æ ¼è¡¨ç¤ºã€‚",
    "å¡è¯OCRè¯†åˆ«": "è¯·è¿›è¡Œå¡è¯ç±»è¯†åˆ«å¹¶ä»¥Markdownè¡¨æ ¼è¾“å‡ºå…³é”®å­—æ®µï¼ˆå¦‚å§“åã€è¯ä»¶å·ã€æœ‰æ•ˆæœŸã€å¡å·ç­‰ï¼‰",
    "ç¥¨æ®OCRè¯†åˆ«": "è¯·è§£æå‘ç¥¨/å°ç¥¨ç­‰ç¥¨æ®ï¼Œè¾“å‡ºå…³é”®ä¿¡æ¯å’Œå¤šè¡Œé¡¹ç›®è¡¨æ ¼ï¼Œå¹¶åœ¨ä¸‹æ–¹ç»™å‡ºç½®ä¿¡åº¦ä¸å¯ç–‘é¡¹ã€‚",
    "åè®®OCRè¯†åˆ«": "è¯·æå–åˆåŒ/åè®®å…³é”®ä¿¡æ¯ï¼ˆç”²ä¹™æ–¹ã€æ—¥æœŸã€é‡‘é¢ã€æ¡æ¬¾ç­‰ï¼‰ï¼Œä¿ç•™æ®µè½ä¸æ¡æ¬¾ç¼–å·ï¼Œå¹¶åœ¨æœ«å°¾ç»™å‡ºé£é™©æç¤ºã€‚",
    "ç©ºé—´åˆ†æ": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»ï¼ŒåŒ…æ‹¬ç›¸å¯¹ä½ç½®ã€è§†è§’ã€é®æŒ¡ã€æ·±åº¦ä¸è·ç¦»æ„Ÿï¼Œå¹¶ç»™å‡ºæ•´ä½“å¸ƒå±€æè¿°ã€‚",
    "æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæˆ–æ°›å›´ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚",
}

VISUAL_CODING_PROMPTS = {
    "HTML": "è¯·æ ¹æ®å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„HTMLç»“æ„ä»£ç ï¼ŒåŒ…å«å¿…è¦çš„è¯­ä¹‰æ ‡ç­¾ã€‚è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚",
    "CSS": "è¯·ä¸ºè¯¥å›¾ç‰‡å¯¹åº”çš„ç•Œé¢ç”Ÿæˆåˆç†çš„CSSæ ·å¼ä»£ç ï¼ŒåŒ…æ‹¬å¸ƒå±€ä¸é¢œè‰²ã€‚è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚",
    "JavaScript": "è¯·æ ¹æ®å›¾ç‰‡äº¤äº’ç”ŸæˆJavaScriptä»£ç ç¤ºä¾‹ï¼ŒåŒ…å«å¿…è¦çš„äº‹ä»¶ä¸é€»è¾‘ã€‚è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚",
    "Python": "è¯·ç”Ÿæˆèƒ½å¤ç°è¯¥ç•Œé¢/å¸ƒå±€çš„Pythonç¤ºä¾‹ä»£ç ï¼ˆå¦‚ä½¿ç”¨streamlitæˆ–flaskçš„ä¼ªä»£ç ï¼‰ã€‚è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚",
}


def _plain_text_to_html(text: str) -> str:
    if not text:
        return ""
    escaped = html.escape(str(text))
    replaced = escaped.replace("\n", "<br>")
    return f'<div class="stats-text">{replaced}</div>'


def _get_default_prompt(task: str, code_format: str = None) -> str:
    if task == "è§†è§‰ç¼–ç¨‹":
        fmt = code_format or "HTML"
        return VISUAL_CODING_PROMPTS.get(fmt, VISUAL_CODING_PROMPTS["HTML"])
    return DEFAULT_TASK_PROMPTS.get(task, DEFAULT_TASK_PROMPTS["ä»»åŠ¡é—®ç­”"])


# å•ä¾‹åº”ç”¨
app = AdvancedQwen3VLApp()

# ä¼šè¯çº§å›¾ç‰‡ä¿å­˜ç›®å½•ä¸è½¨è¿¹
IMAGE_SAVE_ROOT = "chat_history/images"
SESSION_IMAGE_DIR = os.path.join(IMAGE_SAVE_ROOT, getattr(app, "session_id", datetime.now().strftime("%Y%m%d_%H%M%S")))
os.makedirs(SESSION_IMAGE_DIR, exist_ok=True)
app.session_turn_image_paths = []  # ä¸å¯¹è¯è½®æ¬¡å¯¹é½çš„å›¾ç‰‡è·¯å¾„ï¼ˆæ— å›¾åˆ™ä¸º Noneï¼‰


def _toggle_mode(mode, current_task, current_code_format):
    """æ ¹æ®æ¨¡å¼åˆ‡æ¢ç»„ä»¶å¯è§æ€§ï¼Œå¹¶é¢„å¡«å……é»˜è®¤æç¤ºã€‚"""
    is_pro = (mode == "ä¸“ä¸šç‰ˆ")
    task_value = current_task if is_pro else "ä»»åŠ¡é—®ç­”"
    code_visible = is_pro and task_value == "è§†è§‰ç¼–ç¨‹"
    text_value = _get_default_prompt(task_value, current_code_format) if is_pro else ""
    rag_visible = is_pro and task_value == "å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰"
    return (
        gr.update(visible=is_pro),                       # adv_params_box
        gr.update(visible=is_pro),                       # stats_output
        gr.update(visible=is_pro),                       # tab_batch
        gr.update(visible=is_pro),                       # tab_compare
        gr.update(visible=is_pro, value=task_value),     # pro_task dropdown
        gr.update(visible=code_visible),                 # code_format dropdown
        gr.update(visible=rag_visible),                  # rag_feature_selector
        gr.update(value=text_value),                     # text_input prompt
    )


def _toggle_task(task, code_format):
    """ä»»åŠ¡åˆ‡æ¢æ—¶è°ƒæ•´ä»£ç ä¸‹æ‹‰å¯è§æ€§å¹¶é¢„å¡«æç¤ºã€‚"""
    is_visual = (task == "è§†è§‰ç¼–ç¨‹")
    prompt = _get_default_prompt(task, code_format)
    code_kwargs = {"visible": is_visual}
    if is_visual and not code_format:
        code_kwargs["value"] = "HTML"
    rag_visible = (task == "å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰")
    return gr.update(**code_kwargs), gr.update(value=prompt), gr.update(visible=rag_visible)


def _update_code_prompt(task, code_format):
    if task != "è§†è§‰ç¼–ç¨‹":
        return gr.update()
    return gr.update(value=_get_default_prompt(task, code_format))


def handle_unified_chat(image,
                        text,
                        history,
                        max_tokens,
                        temperature,
                        top_p,
                        top_k,
                        mode,
                        pro_task,
                        rag_feature_mode,
                        code_format,
                        repetition_penalty,
                        presence_penalty):
    """ç»Ÿä¸€çš„å‘é€å¤„ç†ï¼š
    - é€šç”¨ç‰ˆï¼šæ™®é€šé—®ç­”
    - ä¸“ä¸šç‰ˆï¼šæŒ‰ä»»åŠ¡åˆ†æ´¾åˆ°ä¸åŒæ–¹æ³•
    è¿”å›: history, cleared_text, stats
    """
    user_text = (text or "").strip()
    saved_image_path = None
    image_digest = None
    if image is not None:
        try:
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            image_digest = hashlib.md5(buffer.getvalue()).hexdigest()
        except Exception:
            image_digest = None

        should_save = image_digest is None or image_digest != getattr(app, "last_image_digest", None)
        if should_save:
            try:
                ts = datetime.now().strftime("%H%M%S%f")
                saved_image_path = os.path.join(SESSION_IMAGE_DIR, f"img_{ts}.png")
                image.save(saved_image_path)
                if image_digest is not None:
                    app.last_image_digest = image_digest
            except Exception:
                saved_image_path = None

    prev_turns = len(history)
    image_recorded = False

    def record_image_path():
        nonlocal image_recorded
        if image_recorded:
            return
        if saved_image_path and saved_image_path != getattr(app, "last_saved_image_path", None):
            app.session_turn_image_paths.append(saved_image_path)
            app.last_saved_image_path = saved_image_path
            if image_digest is not None:
                app.last_image_digest = image_digest
        else:
            existing = getattr(app, "last_saved_image_path", None)
            app.session_turn_image_paths.append(existing)
        image_recorded = True

    try:
        if mode == "é€šç”¨ç‰ˆ":
            effective_prompt = user_text
            chat_result = app.chat_with_image(
                image,
                effective_prompt,
                history,
                max_tokens,
                temperature,
                top_p,
                top_k,
                repetition_penalty,
                presence_penalty
            )

            if inspect.isgenerator(chat_result):
                for out_history, cleared, stats in chat_result:
                    if not image_recorded and len(out_history) > prev_turns:
                        record_image_path()
                    app.chat_history = out_history
                    button_update = gr.update(interactive=bool(app.last_ocr_markdown))
                    stats_update = gr.update(value=_plain_text_to_html(stats), visible=True)
                    yield out_history, cleared, stats_update, button_update, gr.update(value="", visible=True)
            else:
                out_history, cleared, stats = chat_result
                if not image_recorded and len(out_history) > prev_turns:
                    record_image_path()
                app.chat_history = out_history
                button_update = gr.update(interactive=bool(app.last_ocr_markdown))
                stats_update = gr.update(value=_plain_text_to_html(stats), visible=True)
                yield out_history, cleared, stats_update, button_update, gr.update(value="", visible=True)

        else:
            task = pro_task or "ä»»åŠ¡é—®ç­”"
            if task == "OCRè¯†åˆ«":
                if image is None:
                    stats_update = gr.update(value=_plain_text_to_html("âŒ è¯·ä¸Šä¼ å›¾åƒï¼"), visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
                    return

                result = app.ocr_analysis(image)

                if result.startswith("âŒ"):
                    stats_update = gr.update(value="", visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), result
                    return

                prompt_text = user_text if user_text else _get_default_prompt(task, code_format)
                updated_history = history + [[f"ğŸ‘¤ {prompt_text}", result]]
                app.chat_history = updated_history
                if not image_recorded:
                    record_image_path()
                ocr_preview = app.last_ocr_html or _plain_text_to_html(app.last_ocr_markdown or "")
                stats_update = gr.update(value=ocr_preview, visible=True)
                status_update = "âœ… OCRè¯†åˆ«å®Œæˆï¼Œå¯å¯¼å‡ºæ ·å¼"
                yield updated_history, "", stats_update, gr.update(interactive=bool(app.last_ocr_markdown)), status_update
                return

            if task == "å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰":
                app.set_card_api_feature_mode(rag_feature_mode)
                if image is None:
                    stats_update = gr.update(value=_plain_text_to_html("âŒ è¯·ä¸Šä¼ å›¾åƒï¼"), visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
                    return
                result = app.ocr_card_api(image)
                if result.startswith("âŒ"):
                    stats_update = gr.update(value="", visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), result
                    return
                prompt_text = user_text if user_text else _get_default_prompt("å¡è¯OCRè¯†åˆ«", code_format)
                updated_history = history + [[f"ğŸ‘¤ {prompt_text}", result]]
                app.chat_history = updated_history
                if not image_recorded:
                    record_image_path()
                ocr_preview = app.last_ocr_html or _plain_text_to_html(app.last_ocr_markdown or "")
                stats_update = gr.update(value=ocr_preview, visible=True)
                yield updated_history, "", stats_update, gr.update(interactive=bool(app.last_ocr_markdown)), "âœ… å¡è¯OCRè¯†åˆ«(API)å®Œæˆï¼Œå¯å¯¼å‡ºæ ·å¼"
                return

            if task == "å¡è¯OCRè¯†åˆ«":
                if image is None:
                    stats_update = gr.update(value=_plain_text_to_html("âŒ è¯·ä¸Šä¼ å›¾åƒï¼"), visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
                    return
                result = app.ocr_card(image)
                if result.startswith("âŒ"):
                    stats_update = gr.update(value="", visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), result
                    return
                prompt_text = user_text if user_text else _get_default_prompt(task, code_format)
                updated_history = history + [[f"ğŸ‘¤ {prompt_text}", result]]
                app.chat_history = updated_history
                if not image_recorded:
                    record_image_path()
                ocr_preview = app.last_ocr_html or _plain_text_to_html(app.last_ocr_markdown or "")
                stats_update = gr.update(value=ocr_preview, visible=True)
                yield updated_history, "", stats_update, gr.update(interactive=bool(app.last_ocr_markdown)), "âœ… å¡è¯OCRè¯†åˆ«å®Œæˆï¼Œå¯å¯¼å‡ºæ ·å¼"
                return

            if task == "ç¥¨æ®OCRè¯†åˆ«":
                if image is None:
                    stats_update = gr.update(value=_plain_text_to_html("âŒ è¯·ä¸Šä¼ å›¾åƒï¼"), visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
                    return
                result = app.ocr_receipt(image)
                if result.startswith("âŒ"):
                    stats_update = gr.update(value="", visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), result
                    return
                prompt_text = user_text if user_text else _get_default_prompt(task, code_format)
                updated_history = history + [[f"ğŸ‘¤ {prompt_text}", result]]
                app.chat_history = updated_history
                if not image_recorded:
                    record_image_path()
                ocr_preview = app.last_ocr_html or _plain_text_to_html(app.last_ocr_markdown or "")
                stats_update = gr.update(value=ocr_preview, visible=True)
                yield updated_history, "", stats_update, gr.update(interactive=bool(app.last_ocr_markdown)), "âœ… ç¥¨æ®OCRè¯†åˆ«å®Œæˆï¼Œå¯å¯¼å‡ºæ ·å¼"
                return

            if task == "åè®®OCRè¯†åˆ«":
                if image is None:
                    stats_update = gr.update(value=_plain_text_to_html("âŒ è¯·ä¸Šä¼ å›¾åƒï¼"), visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
                    return
                result = app.ocr_agreement(image)
                if result.startswith("âŒ"):
                    stats_update = gr.update(value="", visible=True)
                    yield history, text, stats_update, gr.update(interactive=False), result
                    return
                prompt_text = user_text if user_text else _get_default_prompt(task, code_format)
                updated_history = history + [[f"ğŸ‘¤ {prompt_text}", result]]
                app.chat_history = updated_history
                if not image_recorded:
                    record_image_path()
                ocr_preview = app.last_ocr_html or _plain_text_to_html(app.last_ocr_markdown or "")
                stats_update = gr.update(value=ocr_preview, visible=True)
                yield updated_history, "", stats_update, gr.update(interactive=bool(app.last_ocr_markdown)), "âœ… åè®®OCRè¯†åˆ«å®Œæˆï¼Œå¯å¯¼å‡ºæ ·å¼"
                return

            effective_prompt = user_text if user_text else _get_default_prompt(task, code_format)
            chat_result = app.chat_with_image(
                image,
                effective_prompt,
                history,
                max_tokens,
                temperature,
                top_p,
                top_k,
                repetition_penalty,
                presence_penalty
            )

            if inspect.isgenerator(chat_result):
                for out_history, cleared, stats in chat_result:
                    if not image_recorded and len(out_history) > prev_turns:
                        record_image_path()
                    app.chat_history = out_history
                    button_update = gr.update(interactive=bool(app.last_ocr_markdown))
                    stats_update = gr.update(value=_plain_text_to_html(stats), visible=True)
                    yield out_history, cleared, stats_update, button_update, gr.update()
            else:
                out_history, cleared, stats = chat_result
                if not image_recorded and len(out_history) > prev_turns:
                    record_image_path()
                app.chat_history = out_history
                button_update = gr.update(interactive=bool(app.last_ocr_markdown))
                stats_update = gr.update(value=_plain_text_to_html(stats), visible=True)
                yield out_history, cleared, stats_update, button_update, gr.update()

        if not image_recorded and len(app.chat_history) > prev_turns:
            record_image_path()

    except Exception as e:
        history.append(["ğŸ‘¤", f"âŒ é”™è¯¯: {str(e)}"])
        app.chat_history = history
        if not image_recorded and len(history) > prev_turns:
            record_image_path()
        button_update = gr.update(interactive=bool(app.last_ocr_markdown))
        stats_update = gr.update(value=_plain_text_to_html(f"âŒ é”™è¯¯: {str(e)}"), visible=True)
        yield history, text, stats_update, button_update, f"âŒ é”™è¯¯: {str(e)}"


def save_chat_to_folder(save_dir, history):
    """å°†å½“å‰èŠå¤©å†å²ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ï¼ˆJSONï¼‰ã€‚"""
    try:
        if not save_dir:
            return "âŒ ä¿å­˜å¤±è´¥ï¼šæœªæŒ‡å®šä¿å­˜ç›®å½•"
        os.makedirs(save_dir, exist_ok=True)
        # æ¯æ¬¡ä¿å­˜ä½¿ç”¨ç‹¬ç«‹å¯¼å‡ºå­ç›®å½•ï¼Œé¿å…å›¾ç‰‡ç´¯ç§¯åˆ°åŒä¸€ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_dir = os.path.join(save_dir, timestamp)
        os.makedirs(export_dir, exist_ok=True)
        images_target_dir = os.path.join(export_dir, "images")
        os.makedirs(images_target_dir, exist_ok=True)

        image_paths = getattr(app, "session_turn_image_paths", [])
        copied_rel_paths = []
        seen_images = set()
        for p in image_paths:
            abs_path = os.path.abspath(p) if p else None
            if not p or not os.path.exists(p) or abs_path in seen_images:
                copied_rel_paths.append(None)
                continue
            try:
                basename = os.path.basename(p)
                target = os.path.join(images_target_dir, basename)
                if os.path.abspath(p) != os.path.abspath(target):
                    shutil.copy2(p, target)
                seen_images.add(abs_path)
                copied_rel_paths.append(os.path.join("images", basename))
            except Exception:
                copied_rel_paths.append(None)
        filename = os.path.join(export_dir, f"chat_history_{timestamp}.json")
        # history æ˜¯ [(user, bot), ...]
        data = []
        turns = history or []
        for idx, pair in enumerate(turns):
            try:
                u, b = pair
            except Exception:
                u, b = pair, ""
            img_rel = copied_rel_paths[idx] if idx < len(copied_rel_paths) else None
            data.append({"user": u, "assistant": b, "image_path": img_rel})
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return f"âœ… å·²ä¿å­˜åˆ°: {filename}"
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"


def create_unified_interface():
    """åˆ›å»ºç»Ÿä¸€Gradioç•Œé¢ã€‚"""

    touch_css = """
    :root {
        --radius-lg: 22px;
        --radius-md: 14px;
        --surface: #ffffff;
        --surface-muted: #f5f7fb;
        --surface-border: #e2e8f0;
        --text-primary: #0f172a;
        --text-secondary: #64748b;
        --accent: #2563eb;
        --accent-soft: rgba(37, 99, 235, 0.12);
    }
    body {
        background: linear-gradient(135deg, #eef2ff 0%, #f9fafc 55%, #ffffff 100%);
        color: var(--text-primary);
    }
    .gradio-container {
        max-width: 1650px !important;
        margin: 0 auto;
        padding: 20px 24px 48px;
        font-size: 16px;
        color: var(--text-primary);
    }
    .gradio-container .gr-markdown {
        color: var(--text-primary);
    }
    #unified-header {
        background: linear-gradient(130deg, rgba(37, 99, 235, 0.12), rgba(59, 130, 246, 0.1));
        border: 1px solid rgba(37, 99, 235, 0.18);
        padding: 24px 28px;
        border-radius: 28px;
        box-shadow: 0 18px 36px rgba(15, 23, 42, 0.08);
        margin-bottom: 22px;
    }
    #unified-header h1 {
        margin: 0 0 6px;
        font-size: 26px;
        font-weight: 600;
        letter-spacing: 0.2px;
    }
    #unified-header p {
        margin: 0;
        color: var(--text-secondary);
    }
    #unified-mode-bar {
        background: var(--surface);
        border-radius: 24px;
        box-shadow: 0 20px 40px rgba(15, 23, 42, 0.06);
        padding: 20px 22px;
        gap: 18px;
        margin-bottom: 20px;
        border: 1px solid var(--surface-border);
    }
    #unified-mode-bar .gradio-button,
    #unified-mode-bar button {
        font-size: 16px !important;
        padding: 12px 18px !important;
        border-radius: 14px !important;
    }
    #unified-mode-bar textarea,
    #unified-mode-bar input[type="text"] {
        background: var(--surface);
        border: 1px solid rgba(148, 163, 184, 0.35);
        border-radius: 14px;
        color: var(--text-primary);
        box-shadow: inset 0 1px 3px rgba(15, 23, 42, 0.04);
    }
    #unified-mode-bar textarea:focus,
    #unified-mode-bar input[type="text"]:focus {
        border-color: var(--accent);
        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.12);
    }
    .gradio-container .tabs {
        background: transparent;
        border: none;
    }
    .gradio-container .tabitem {
        border-radius: var(--radius-md);
        background: #f8fafc;
        border: 1px solid transparent;
        color: var(--text-secondary);
    }
    .gradio-container .tabitem.selected {
        border-color: rgba(37, 99, 235, 0.25);
        color: var(--text-primary);
        background: #ffffff;
        box-shadow: 0 10px 20px rgba(37, 99, 235, 0.08);
    }
    #unified-input-panel,
    #unified-chat-panel,
    #unified-batch-panel,
    #unified-compare-panel {
        background: var(--surface);
        border-radius: 24px;
        padding: 22px 24px;
        box-shadow: 0 22px 44px rgba(15, 23, 42, 0.06);
        border: 1px solid var(--surface-border);
    }
    #unified-input-panel .gradio-slider > label,
    #unified-input-panel .gradio-dropdown > label {
        color: var(--text-secondary);
    }
    #unified-chat-panel {
        display: flex;
        flex-direction: column;
        gap: 16px;
    }
    #unified-chatbot > .wrap {
        background: #f8fafc;
        border-radius: 20px;
        border: 1px solid rgba(148, 163, 184, 0.25);
        padding: 8px 10px;
    }
    #unified-chatbot .message {
        border-radius: 16px !important;
        padding: 12px 14px !important;
        line-height: 1.6;
        font-size: 15px;
        color: var(--text-primary);
    }
    #unified-chatbot .message.user {
        background: linear-gradient(138deg, rgba(37, 99, 235, 0.16), rgba(96, 165, 250, 0.12));
        border: 1px solid rgba(37, 99, 235, 0.22);
        color: var(--text-primary);
        align-self: flex-end;
    }
    #unified-chatbot .message.bot {
        background: #ffffff;
        border: 1px solid rgba(203, 213, 225, 0.9);
        color: var(--text-primary);
        align-self: flex-start;
    }
    #unified-query textarea {
        border-radius: 16px;
        border: 1px solid rgba(148, 163, 184, 0.35);
        background: var(--surface);
        color: var(--text-primary);
        box-shadow: inset 0 1px 3px rgba(15, 23, 42, 0.05);
    }
    #unified-query textarea:focus {
        border-color: var(--accent);
        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.12);
    }
    #unified-stats .stats-text {
        background: var(--accent-soft);
        border-radius: 16px;
        border: 1px solid rgba(37, 99, 235, 0.2);
        color: var(--text-primary);
        font-weight: 500;
        padding: 12px 14px;
        line-height: 1.6;
        margin-bottom: 12px;
        word-break: break-word;
    }
    .gradio-container .gradio-button.primary {
        background: linear-gradient(135deg, #2563eb, #1d4ed8);
        border: none;
        color: #ffffff;
        font-weight: 600;
        box-shadow: 0 18px 30px rgba(37, 99, 235, 0.22);
    }
    .gradio-container .gradio-button.primary:hover {
        filter: brightness(1.03);
    }
    .gradio-container .gradio-button.secondary {
        background: rgba(37, 99, 235, 0.1);
        border: 1px solid rgba(37, 99, 235, 0.18);
        color: var(--text-primary);
    }
    .gradio-container textarea,
    .gradio-container input[type="text"],
    .gradio-container input[type="number"] {
        background: var(--surface);
        border: 1px solid rgba(148, 163, 184, 0.35);
        color: var(--text-primary);
        border-radius: 16px;
    }
    .gradio-container textarea:focus,
    .gradio-container input[type="text"]:focus,
    .gradio-container input[type="number"]:focus {
        border-color: var(--accent);
        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.12);
    }
    #unified-batch-panel textarea,
    #unified-compare-panel textarea {
        min-height: 320px;
    }
    .gradio-container .dropdown span.label,
    .gradio-container .slider > label,
    .gradio-container .dropdown label {
        color: var(--text-secondary);
    }
    .gradio-container .gradio-dropdown .wrap select,
    .gradio-container .gradio-dropdown .wrap button {
        background: var(--surface);
        color: var(--text-primary);
        border-color: rgba(148, 163, 184, 0.4);
    }
    .gradio-container .gradio-dropdown .wrap select:focus,
    .gradio-container .gradio-dropdown .wrap button:focus {
        border-color: var(--accent);
        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.12);
    }
    /*
    Bigger markdown preview area for unified stats (OCR/table preview)
    */
    #unified-stats {
        max-height: 560px;
        overflow: auto;
        border: 1px solid rgba(148, 163, 184, 0.35);
        padding: 12px 14px;
        border-radius: 14px;
        background: #ffffff;
    }
    #unified-stats table {
        width: 100%;
        border-collapse: collapse;
        margin: 8px 0 14px;
    }
    #unified-stats th,
    #unified-stats td {
        border: 1px solid #e5e7eb;
        padding: 8px 10px;
        text-align: left;
        vertical-align: top;
        font-size: 14px;
        line-height: 1.55;
    }
    #unified-stats thead th {
        background: #f8fafc;
        font-weight: 600;
    }
    #unified-stats code {
        background: #f8fafc;
        border: 1px solid #e5e7eb;
        padding: 1px 4px;
        border-radius: 6px;
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    }
    """

    with gr.Blocks(
        title="å¤šæ¨¡æ€å¤§æ¨¡å‹æ™ºèƒ½åŠ©æ‰‹",
        theme=gr.themes.Soft(),
        css=touch_css
    ) as interface:

        gr.HTML("""
        <section id="unified-header">
          <h1>ğŸ¤– å¤šæ¨¡æ€å¤§æ¨¡å‹æ™ºèƒ½åŠ©æ‰‹</h1>
          <p>å…¨æ–°å¸ƒå±€ä¸å¯¹è¯æ ·å¼ï¼Œé€šç”¨ / ä¸“ä¸šåŒæ¨¡å¼éšå¿ƒåˆ‡æ¢ï¼Œæ”¯æŒä»»åŠ¡åˆ†æ´¾ä¸å¯¹è¯ä¿å­˜ã€‚</p>
        </section>
        """)

        with gr.Row(elem_id="unified-mode-bar"):
            with gr.Column(scale=1, min_width=240):
                mode = gr.Radio(
                    choices=["é€šç”¨ç‰ˆ", "ä¸“ä¸šç‰ˆ"], value="é€šç”¨ç‰ˆ", label="ç•Œé¢æ¨¡å¼"
                )
                pro_task = gr.Dropdown(
                    choices=["ä»»åŠ¡é—®ç­”", "OCRè¯†åˆ«", "å¡è¯OCRè¯†åˆ«", "å¡è¯OCRè¯†åˆ«ï¼ˆAPIï¼‰", "ç¥¨æ®OCRè¯†åˆ«", "åè®®OCRè¯†åˆ«", "ç©ºé—´åˆ†æ", "è§†è§‰ç¼–ç¨‹", "æƒ…æ„Ÿåˆ†æ"],
                    value="ä»»åŠ¡é—®ç­”",
                    label="ä¸“ä¸šä»»åŠ¡",
                    visible=False,
                )
                rag_feature_selector = gr.Radio(
                    choices=["æ ·å¼ç‰¹å¾RAG", "CLIPå›¾åƒç‰¹å¾"],
                    value="æ ·å¼ç‰¹å¾RAG",
                    label="å¡è¯RAGç‰¹å¾æ¨¡å¼",
                    visible=False,
                )
            with gr.Column(scale=1, min_width=240):
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                status_text = gr.Textbox(
                    label="è¿è¡ŒçŠ¶æ€",
                    value="â³ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç‚¹å‡»åŠ è½½æ¨¡å‹æŒ‰é’®",
                    interactive=False,
                    lines=3,
                )
            with gr.Column(scale=1, min_width=240):
                save_btn = gr.Button("ğŸ’¾ ä¿å­˜å½“å‰å¯¹è¯", variant="secondary")
                save_dir = gr.Textbox(value="chat_history", label="ä¿å­˜ç›®å½•", interactive=False)

        load_btn.click(app.load_model, outputs=[status_text, load_btn])

        # æ ·å¼åœ¨ Blocks å®ä¾‹åŒ–æ—¶åº”ç”¨ï¼Œæ— éœ€è¿è¡Œæ—¶åˆ‡æ¢

        with gr.Tab("ğŸ’¬ å›¾åƒå¯¹è¯"):
            with gr.Row(equal_height=True):
                with gr.Column(scale=1):
                    with gr.Group(elem_id="unified-input-panel"):
                        gr.Markdown("### å›¾åƒä¸å‚æ•°è®¾ç½®")
                        image_input = gr.Image(label="ä¸Šä¼ å›¾åƒ", type="pil", height=400)

                        # é€šç”¨å‚æ•°
                        with gr.Row(equal_height=True):
                            max_tokens = gr.Slider(
                                minimum=512, maximum=16384, value=8192, label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (out_seq_length)"
                            )
                            temperature = gr.Slider(
                                minimum=0.0, maximum=2.0, value=0.7, label="åˆ›é€ æ€§ (temperature)"
                            )

                        # ä¸“ä¸šå‚æ•°å®¹å™¨ï¼ˆé»˜è®¤éšè—ï¼‰
                        with gr.Accordion("ğŸ›ï¸ é«˜çº§å‚æ•°", open=False, visible=False) as adv_params_box:
                            top_p = gr.Slider(
                                minimum=0.0, maximum=1.0, value=0.8, label="top_p"
                            )
                            top_k = gr.Slider(
                                minimum=0, maximum=100, value=20, label="top_k"
                            )
                            repetition_penalty = gr.Slider(
                                minimum=0.8, maximum=2.0, value=1.0, step=0.05, label="repetition_penalty"
                            )
                            presence_penalty = gr.Slider(
                                minimum=0.0, maximum=3.0, value=1.5, step=0.1, label="presence_penalty (å ä½)"
                            )

                with gr.Column(scale=2):
                    with gr.Group(elem_id="unified-chat-panel"):
                        gr.Markdown("### å¯¹è¯ä¸è¾“å‡º")
                        chatbot = gr.Chatbot(
                            label=None,
                            height=500,
                            show_label=False,
                            type="tuples",
                            elem_id="unified-chatbot",
                            render_markdown=True
                        )
                        text_input = gr.Textbox(
                            label=None,
                            placeholder="è¾“å…¥æƒ³äº†è§£çš„å†…å®¹ï¼ŒæŒ‰ Enter æˆ–ç‚¹å‡»å‘é€ã€‚",
                            lines=3,
                            elem_id="unified-query"
                        )
                        with gr.Row():
                            send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", variant="secondary", scale=1)
                        with gr.Row():
                            with gr.Column(scale=4):
                                stats_output = gr.HTML(
                                    value="",
                                    visible=False,
                                    elem_id="unified-stats"
                                )
                            with gr.Column(scale=1, min_width=220):
                                ocr_export_btn = gr.Button("ğŸ’¾ å¯¼å‡ºæ ·å¼", variant="secondary", interactive=False)
                                ocr_export_status = gr.Textbox(
                                    label="å¯¼å‡ºçŠ¶æ€",
                                    interactive=False,
                                    lines=4
                                )
                code_format = gr.Dropdown(
                    choices=["HTML", "CSS", "JavaScript", "Python"],
                    value="HTML",
                    label="ä»£ç ç±»å‹",
                    visible=False,
                )

            # é€šç”¨/ä¸“ä¸šä¸¤ç§è°ƒç”¨è·¯å¾„ï¼ˆåˆ©ç”¨åŒä¸€é«˜çº§åº”ç”¨ï¼Œä¸“ä¸šå¤šä¸¤ä¸ªå‚æ•°ä¸ç»Ÿè®¡è¾“å‡ºï¼‰
            send_btn.click(
                handle_unified_chat,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature, top_p, top_k, mode, pro_task, rag_feature_selector, code_format, repetition_penalty, presence_penalty],
                outputs=[chatbot, text_input, stats_output, ocr_export_btn, ocr_export_status],
            )
            text_input.submit(
                handle_unified_chat,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature, top_p, top_k, mode, pro_task, rag_feature_selector, code_format, repetition_penalty, presence_penalty],
                outputs=[chatbot, text_input, stats_output, ocr_export_btn, ocr_export_status],
            )
            def _clear_session():
                app.clear_history()
                return [], "", gr.update(value="", visible=False), gr.update(interactive=False), ""

            clear_btn.click(
                _clear_session,
                outputs=[chatbot, text_input, stats_output, ocr_export_btn, ocr_export_status],
            )
        save_btn.click(save_chat_to_folder, inputs=[save_dir, chatbot], outputs=[status_text])

        ocr_export_btn.click(
            app.export_last_ocr,
            outputs=[ocr_export_status],
        )

        # é«˜çº§åŠŸèƒ½Tabï¼ˆé»˜è®¤éšè—ï¼Œé€šè¿‡æ¨¡å¼åˆ‡æ¢æ˜¾ç¤ºï¼‰
        with gr.Tab("ğŸ“Š æ‰¹é‡åˆ†æ", visible=False) as tab_batch:
            with gr.Group(elem_id="unified-batch-panel"):
                with gr.Row():
                    with gr.Column():
                        batch_images = gr.File(
                            label="ä¸Šä¼ å¤šä¸ªå›¾åƒ", file_count="multiple", file_types=["image"]
                        )
                        analysis_type = gr.Dropdown(
                            choices=["æè¿°", "OCR", "ç©ºé—´åˆ†æ", "æƒ…æ„Ÿåˆ†æ"], value="æè¿°", label="åˆ†æç±»å‹"
                        )
                        batch_btn = gr.Button("ğŸ” å¼€å§‹æ‰¹é‡åˆ†æ", variant="primary")
                    with gr.Column():
                        batch_result = gr.Markdown()
            batch_btn.click(app.batch_analysis, inputs=[batch_images, analysis_type], outputs=[batch_result])

        with gr.Tab("ğŸ”„ å›¾åƒå¯¹æ¯”", visible=False) as tab_compare:
            with gr.Group(elem_id="unified-compare-panel"):
                with gr.Row():
                    with gr.Column():
                        compare_image1 = gr.Image(label="å›¾åƒ1", type="pil", height=220)
                        compare_image2 = gr.Image(label="å›¾åƒ2", type="pil", height=220)
                        comparison_type = gr.Dropdown(
                            choices=["ç›¸ä¼¼æ€§", "é£æ ¼", "å†…å®¹", "ç»¼åˆ"], value="ç›¸ä¼¼æ€§", label="å¯¹æ¯”ç±»å‹"
                        )
                        compare_btn = gr.Button("ğŸ”„ å¼€å§‹å¯¹æ¯”", variant="primary")
                    with gr.Column():
                        compare_result = gr.Markdown()
            compare_btn.click(
                app.compare_images,
                inputs=[compare_image1, compare_image2, comparison_type],
                outputs=[compare_result],
            )

        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown(
                """
                - å…ˆç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€åå†ä½¿ç”¨å„é¡¹åŠŸèƒ½ã€‚
                - ã€Œé€šç”¨ç‰ˆã€é€‚åˆå¿«é€Ÿä¸Šæ‰‹ä¸æ—¥å¸¸ä½¿ç”¨ï¼›ã€Œä¸“ä¸šç‰ˆã€æä¾›æ›´ç»†ç²’åº¦çš„ç”Ÿæˆå‚æ•°ä¸é«˜çº§åŠŸèƒ½ï¼ˆæ‰¹é‡åˆ†æã€å›¾åƒå¯¹æ¯”ï¼‰ã€‚
                - å·²é»˜è®¤ä¼˜åŒ–ä¸ºæ›´æ˜“è§¦æ‘¸ç‚¹å‡»çš„ç•Œé¢å°ºå¯¸ã€‚
                """
            )

        # ç»‘å®šæ¨¡å¼åˆ‡æ¢ï¼šæ§åˆ¶é«˜çº§ç»„ä»¶å¯è§æ€§
        mode.change(
            _toggle_mode,
            inputs=[mode, pro_task, code_format],
            outputs=[adv_params_box, stats_output, tab_batch, tab_compare, pro_task, code_format, rag_feature_selector, text_input],
        )

        pro_task.change(
            _toggle_task,
            inputs=[pro_task, code_format],
            outputs=[code_format, text_input, rag_feature_selector],
        )

        code_format.change(
            _update_code_prompt,
            inputs=[pro_task, code_format],
            outputs=[text_input],
        )

    return interface


def main():
    print("ğŸš€ å¯åŠ¨Qwen3-VL-8B-Instruct ç»Ÿä¸€Webç•Œé¢...")
    interface = create_unified_interface()

    def _cleanup():
        # æ¸…ç†æ¨¡å‹ä¸æ˜¾å­˜
        try:
            app.model = None
            app.processor = None
            app.is_loaded = False
        except Exception:
            pass
        try:
            if torch is not None and hasattr(torch, "cuda") and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
        except Exception:
            pass
        try:
            gc.collect()
        except Exception:
            pass

    # æ³¨å†Œè¿›ç¨‹é€€å‡ºæ¸…ç†
    atexit.register(_cleanup)
    interface.queue()
    interface.launch(
        server_name="127.0.0.1",
        server_port=None,  # è‡ªåŠ¨é€‰æ‹©å¯ç”¨ç«¯å£ï¼Œé¿å…ç«¯å£å ç”¨é”™è¯¯
        share=False,
        debug=True,
        show_error=True,
    )


if __name__ == "__main__":
    main()
