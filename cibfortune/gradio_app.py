#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-VL-8B-Instruct Gradio Webç•Œé¢
æä¾›å‹å¥½çš„Webç•Œé¢æ¥ä½¿ç”¨Qwen3-VLæ¨¡å‹
"""

import os
import gradio as gr
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from PIL import Image
import requests
from io import BytesIO
import time
import json
import csv
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class Qwen3VLGradioApp:
    """Qwen3-VL Gradioåº”ç”¨ç±»"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.model_path = "D:\cibfortune\Cibfortune\cibfortune\models\qwen3-vl-2b-instruct"
        self.is_loaded = False
        self.chat_history = []
        self.chat_messages = []
        self.last_image = None
        self.last_ocr_markdown = None

    def _sanitize_markdown(self, text: str) -> str:
        if not text:
            return ""
        s = text.strip()
        lines = s.splitlines()
        out = []
        in_fence = False
        for line in lines:
            stripped = line.strip()
            if stripped.startswith("```"):
                in_fence = not in_fence
                continue
            out.append(line) if not in_fence else None
        cleaned = "\n".join(out).strip()
        return cleaned if cleaned else s
        
    def load_model(self, progress=gr.Progress()):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            return "âœ… æ¨¡å‹å·²ç»åŠ è½½å®Œæˆï¼"
        
        try:
            progress(0.1, desc="æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
            if not os.path.exists(self.model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}"
            
            progress(0.3, desc="åŠ è½½æ¨¡å‹...")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype="auto",
                device_map="cuda",
                load_in_4bit=True,
            )
            
            progress(0.7, desc="åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            
            progress(1.0, desc="å®Œæˆï¼")
            self.is_loaded = True
            
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚"
            
        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def _prepare_user_message(self, image, prompt):
        prompt_clean = (prompt or "").strip()
        resolved_image = image if image is not None else self.last_image
        if resolved_image is None:
            raise ValueError("âŒ è¯·ä¸Šä¼ å›¾åƒï¼")
        if not prompt_clean:
            raise ValueError("âŒ è¯·è¾“å…¥é—®é¢˜ï¼")
        if image is not None:
            self.last_image = image
        content = [
            {"type": "image", "image": resolved_image},
            {"type": "text", "text": prompt_clean},
        ]
        return prompt_clean, {"role": "user", "content": content}

    def _run_inference(self, image, prompt, max_tokens, temperature, prepared=None):
        if prepared is None:
            prompt_clean, user_message = self._prepare_user_message(image, prompt)
        else:
            prompt_clean, user_message = prepared
        messages = self.chat_messages + [user_message]
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.model.device)

        generation_kwargs = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "do_sample": True if temperature > 0 else False
        }

        start_time = time.time()
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, **generation_kwargs)
        generation_time = time.time() - start_time

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        response = output_text[0]

        assistant_message = {"role": "assistant", "content": [{"type": "text", "text": response}]}
        self.chat_messages.extend([user_message, assistant_message])
        return prompt_clean, response, generation_time

    def _clone_history(self, history):
        return [[turn[0], turn[1]] for turn in history]

    def _chunk_response(self, text, chunk_size=80):
        if not text:
            return []
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    @staticmethod
    def _parse_markdown_sections(markdown_text):
        sections = []
        lines = markdown_text.splitlines()
        i = 0

        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            is_table = (
                stripped.startswith("|")
                and stripped.count("|") >= 2
                and i + 1 < len(lines)
                and set(lines[i + 1].replace("|", "").strip()) <= set("-: ")
            )

            if is_table:
                header = [cell.strip() for cell in stripped.strip("|").split("|")]
                i += 2
                rows = []
                while i < len(lines):
                    row_line = lines[i].strip()
                    if not (row_line.startswith("|") and row_line.count("|") >= 2):
                        break
                    row = [cell.strip() for cell in row_line.strip("|").split("|")]
                    rows.append(row)
                    i += 1
                sections.append({"type": "table", "header": header, "rows": rows})
                continue

            text_block = []
            while i < len(lines):
                current = lines[i]
                stripped_current = current.strip()
                next_is_table = (
                    stripped_current.startswith("|")
                    and stripped_current.count("|") >= 2
                    and i + 1 < len(lines)
                    and set(lines[i + 1].replace("|", "").strip()) <= set("-: ")
                )
                if next_is_table:
                    break
                text_block.append(current)
                i += 1
                if i < len(lines) and lines[i] == "":
                    text_block.append(lines[i])
            text_content = "\n".join(text_block).strip("\n")
            if text_content:
                sections.append({"type": "text", "text": text_content})

        return sections

    def chat_with_image(self, image, text, history, max_tokens, temperature):
        """ä¸å›¾åƒå¯¹è¯ï¼ˆæµå¼åé¦ˆï¼‰"""
        original_text = text

        if not self.is_loaded:
            yield history, original_text
            return

        try:
            prepared = self._prepare_user_message(image, text)
        except ValueError as exc:
            yield history, original_text
            return

        prompt_clean, _ = prepared
        history_copy = self._clone_history(history)
        history_copy.append([f"ğŸ‘¤ {prompt_clean}", "ğŸ¤– æ­£åœ¨æ€è€ƒ..."])
        yield self._clone_history(history_copy), original_text

        try:
            _, response, _ = self._run_inference(image, text, max_tokens, temperature, prepared=prepared)
        except Exception as e:
            history_copy[-1][1] = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            self.chat_history = self._clone_history(history_copy)
            yield self._clone_history(history_copy), original_text
            return

        assembled = ""
        chunks = self._chunk_response(response)
        if not chunks:
            chunks = [""]
        for chunk in chunks:
            assembled += chunk
            history_copy[-1][1] = f"ğŸ¤– {assembled}â–Œ"
            yield self._clone_history(history_copy), original_text

        history_copy[-1][1] = f"ğŸ¤– {response}"
        final_history = self._clone_history(history_copy)
        self.chat_history = final_history
        yield final_history, original_text
    
    def ocr_analysis(self, image, prompt: str = None):
        """OCRæ–‡å­—è¯†åˆ«"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        default_prompt = "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚å¦‚æœå›¾ç‰‡ä¸­æœ‰å¤šç§è¯­è¨€ï¼Œè¯·åˆ†åˆ«æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚"
        effective_prompt = (prompt or "").strip() or default_prompt
        
        try:
            prompt_clean, response, _ = self._run_inference(image, effective_prompt, max_tokens=1024, temperature=0.7)
            cleaned = self._sanitize_markdown(response)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {cleaned}"])
            self.last_ocr_markdown = f"## OCRè¯†åˆ«ç»“æœ\n\n{cleaned}"
            return f"ğŸ“ OCRè¯†åˆ«ç»“æœ:\n\n{cleaned}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ OCRè¯†åˆ«å¤±è´¥: {str(e)}"
    
    def spatial_analysis(self, image, prompt: str = None):
        """ç©ºé—´æ„ŸçŸ¥åˆ†æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        default_prompt = """è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»ï¼ŒåŒ…æ‹¬ï¼š
            1. ç‰©ä½“çš„ç›¸å¯¹ä½ç½®å…³ç³»
            2. è§†è§’å’Œè§‚å¯Ÿè§’åº¦
            3. ç‰©ä½“çš„é®æŒ¡å…³ç³»
            4. æ·±åº¦å’Œè·ç¦»æ„Ÿ
            5. ç©ºé—´å¸ƒå±€çš„æ•´ä½“æè¿°"""
        effective_prompt = (prompt or "").strip() or default_prompt
        
        try:
            prompt_clean, response, _ = self._run_inference(image, effective_prompt, max_tokens=768, temperature=0.7)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {response}"])
            return f"ğŸ” ç©ºé—´åˆ†æç»“æœ:\n\n{response}"
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ ç©ºé—´åˆ†æå¤±è´¥: {str(e)}"
    
    def visual_coding(self, image, output_format, prompt: str = None):
        """è§†è§‰ç¼–ç¨‹"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"

        try:
            format_prompts = {
                "HTML": "è¯·æ ¹æ®å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„HTMLç»“æ„ä»£ç ï¼ŒåŒ…å«å¿…è¦çš„è¯­ä¹‰æ ‡ç­¾ã€‚",
                "CSS": "è¯·ä¸ºè¯¥å›¾ç‰‡å¯¹åº”çš„ç•Œé¢ç”Ÿæˆåˆç†çš„CSSæ ·å¼ä»£ç ï¼ŒåŒ…æ‹¬å¸ƒå±€ä¸é¢œè‰²ã€‚",
                "JavaScript": "è¯·æ ¹æ®å›¾ç‰‡äº¤äº’ç”ŸæˆJavaScriptä»£ç ç¤ºä¾‹ï¼ŒåŒ…å«å¿…è¦çš„äº‹ä»¶ä¸é€»è¾‘ã€‚",
                "Python": "è¯·ç”Ÿæˆèƒ½å¤ç°è¯¥ç•Œé¢/å¸ƒå±€çš„Pythonç¤ºä¾‹ä»£ç ï¼ˆå¦‚ä½¿ç”¨streamlitæˆ–flaskçš„ä¼ªä»£ç ï¼‰ã€‚",
            }
            base_prompt = format_prompts.get(output_format, format_prompts["HTML"]) + " è¯·åªè¾“å‡ºä»£ç ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚"
            effective_prompt = (prompt or "").strip() or base_prompt

            prompt_clean, response, _ = self._run_inference(image, effective_prompt, max_tokens=2048, temperature=0.4)
            self.chat_history.append([f"ğŸ‘¤ {prompt_clean}", f"ğŸ¤– {response}"])
            return f"ğŸ’» {output_format}ä»£ç :\n\n```{output_format.lower()}\n{response}\n```"
            
        except ValueError as exc:
            return str(exc)
        except Exception as e:
            return f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
        self.chat_messages = []
        self.last_image = None
        self.last_ocr_markdown = None
        return []

    def export_last_ocr(self):
        if not getattr(self, "last_ocr_markdown", None):
            return "âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ–‡æœ¬æ ·å¼ï¼Œè¯·å…ˆæ‰§è¡Œä¸€æ¬¡OCRè¯†åˆ«ï¼"

        export_dir = os.path.join("ocr_exports")
        os.makedirs(export_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        sections = self._parse_markdown_sections(self.last_ocr_markdown)

        excel_path = os.path.join(export_dir, f"ocr_{timestamp}.xlsx")
        excel_note = ""
        try:
            from openpyxl import Workbook

            wb = Workbook()
            ws = wb.active
            ws.title = "è¡¨æ ¼1" if sections else "OCRæ–‡æœ¬"
            table_idx = 0
            for section in sections:
                if section["type"] == "table":
                    table_idx += 1
                    if table_idx > 1:
                        ws = wb.create_sheet(title=f"è¡¨æ ¼{table_idx}")
                    ws.append(section["header"])
                    for row in section["rows"]:
                        ws.append(row)
                elif section["type"] == "text" and section["text"]:
                    if table_idx > 0:
                        ws = wb.create_sheet(title=f"æ–‡æœ¬{table_idx}")
                    for line in section["text"].splitlines():
                        ws.append([line])
            if not sections:
                for line in self.last_ocr_markdown.splitlines():
                    ws.append([line])
            wb.save(excel_path)
        except Exception as exc:
            excel_path = os.path.join(export_dir, f"ocr_{timestamp}.csv")
            with open(excel_path, "w", encoding="utf-8", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["OCR Result"])
                for line in self.last_ocr_markdown.splitlines():
                    writer.writerow([line])
            excel_note = f"âš ï¸ Excelå¯¼å‡ºå¤±è´¥({exc})ï¼Œå·²ä¿å­˜ä¸ºCSV"

        json_path = os.path.join(export_dir, f"ocr_{timestamp}.json")
        json_content = {
            "markdown": self.last_ocr_markdown,
            "sections": sections,
        }
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_content, f, ensure_ascii=False, indent=2)

        message_lines = [
            "âœ… æ–‡æœ¬æ ·å¼å·²ä¿å­˜ï¼š",
            f"- Excel: {excel_path}" + (f" ({excel_note})" if excel_note else ""),
            f"- JSON: {json_path}",
        ]
        return "\n".join(message_lines)

# åˆ›å»ºåº”ç”¨å®ä¾‹
# åˆ›å»ºåº”ç”¨å®ä¾‹
app = Qwen3VLGradioApp()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen3-VL-8B-Instruct Webç•Œé¢",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1600px !important;
        }
        .chat-message {
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
        }
        #ocr-md {
            max-height: 560px;
            overflow: auto;
            border: 1px solid #eee;
            padding: 10px;
            border-radius: 6px;
            background: #fff;
        }
        """
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¤– Qwen3-VL-8B-Instruct Webç•Œé¢
        
        æ¬¢è¿ä½¿ç”¨Qwen3-VL-8B-Instructå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼è¿™ä¸ªç•Œé¢æä¾›äº†å‹å¥½çš„Webäº¤äº’æ–¹å¼ã€‚
        
        **ä¸»è¦åŠŸèƒ½ï¼š**
        - ğŸ–¼ï¸ å›¾åƒç†è§£å’Œå¯¹è¯
        - ğŸ“ OCRæ–‡å­—è¯†åˆ«
        - ğŸ” ç©ºé—´æ„ŸçŸ¥åˆ†æ
        - ğŸ’» è§†è§‰ç¼–ç¨‹ï¼ˆç”Ÿæˆä»£ç ï¼‰
        """)
        
        with gr.Tab("ğŸš€ æ¨¡å‹ç®¡ç†"):
            gr.Markdown("### æ¨¡å‹åŠ è½½")
            with gr.Row():
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
                status_text = gr.Textbox(
                    label="çŠ¶æ€", 
                    value="â³ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç‚¹å‡»åŠ è½½æ¨¡å‹æŒ‰é’®",
                    interactive=False
                )
            
            load_btn.click(
                app.load_model,
                outputs=[status_text]
            )
        
        with gr.Tab("ğŸ’¬ å›¾åƒå¯¹è¯"):
            gr.Markdown("### ä¸å›¾åƒè¿›è¡Œå¯¹è¯")
            
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ å›¾åƒ",
                        type="pil",
                        height=400
                    )
                    
                    with gr.Row():
                        max_tokens = gr.Slider(
                            minimum=50, maximum=1024, value=256,
                            label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                        )
                        temperature = gr.Slider(
                            minimum=0.1, maximum=2.0, value=0.7,
                            label="åˆ›é€ æ€§ (Temperature)"
                        )
                
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=400,
                        show_label=True,
                        render_markdown=True
                    )
                    
                    with gr.Row():
                        text_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            placeholder="è¯·æè¿°è¿™å¼ å›¾ç‰‡...",
                            lines=2
                        )
                        send_btn = gr.Button("å‘é€", variant="primary")
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²")
            
            # äº‹ä»¶ç»‘å®š
            send_btn.click(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature],
                outputs=[chatbot, text_input]
            )
            
            text_input.submit(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature],
                outputs=[chatbot, text_input]
            )
            
        
        with gr.Tab("ğŸ“ OCRè¯†åˆ«"):
            gr.Markdown("### æ–‡å­—è¯†åˆ«")
            
            with gr.Row():
                with gr.Column():
                    ocr_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒè¿›è¡ŒOCRè¯†åˆ«",
                        type="pil",
                        height=300
                    )
                    ocr_btn = gr.Button("ğŸ” å¼€å§‹è¯†åˆ«", variant="primary")
                
                with gr.Column(scale=2):
                    ocr_md = gr.Markdown(
                        value="",
                        elem_id="ocr-md"
                    )
                    save_style_btn = gr.Button("ğŸ’¾ å¯¼å‡ºæ ·å¼", variant="secondary", interactive=False)
                    ocr_export_status = gr.Textbox(
                        label="å¯¼å‡ºçŠ¶æ€",
                        interactive=False,
                        lines=4
                    )

            def _run_ocr(image):
                result = app.ocr_analysis(image)
                can_save = bool(app.last_ocr_markdown) and not result.startswith("âŒ")
                display_md = app.last_ocr_markdown if can_save else ""
                status = "" if can_save else result
                return display_md, gr.update(interactive=can_save), status

            ocr_btn.click(
                _run_ocr,
                inputs=[ocr_image],
                outputs=[ocr_md, save_style_btn, ocr_export_status]
            )

            save_style_btn.click(
                app.export_last_ocr,
                outputs=[ocr_export_status]
            )

        def _clear_all():
            app.clear_history()
            return [], gr.update(interactive=False), ""

        clear_btn.click(
            _clear_all,
            outputs=[chatbot, save_style_btn, ocr_export_status]
        )
        
        with gr.Tab("ğŸ” ç©ºé—´åˆ†æ"):
            gr.Markdown("### ç©ºé—´æ„ŸçŸ¥åˆ†æ")
            
            with gr.Row():
                with gr.Column():
                    spatial_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒè¿›è¡Œç©ºé—´åˆ†æ",
                        type="pil",
                        height=300
                    )
                    spatial_btn = gr.Button("ğŸ” å¼€å§‹åˆ†æ", variant="primary")
                
                with gr.Column():
                    spatial_result = gr.Textbox(
                        label="åˆ†æç»“æœ",
                        lines=15,
                        max_lines=20
                    )
            
            spatial_btn.click(
                app.spatial_analysis,
                inputs=[spatial_image],
                outputs=[spatial_result]
            )
        
        with gr.Tab("ğŸ’» è§†è§‰ç¼–ç¨‹"):
            gr.Markdown("### ä»å›¾åƒç”Ÿæˆä»£ç ")
            
            with gr.Row():
                with gr.Column():
                    code_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒç”Ÿæˆä»£ç ",
                        type="pil",
                        height=300
                    )
                    
                    code_format = gr.Dropdown(
                        choices=["HTML", "CSS", "JavaScript", "Python"],
                        value="HTML",
                        label="é€‰æ‹©ä»£ç ç±»å‹"
                    )
                    
                    code_btn = gr.Button("ğŸ’» ç”Ÿæˆä»£ç ", variant="primary")
                
                with gr.Column():
                    code_result = gr.Textbox(
                        label="ç”Ÿæˆçš„ä»£ç ",
                        lines=15,
                        max_lines=20
                    )
            
            code_btn.click(
                app.visual_coding,
                inputs=[code_image, code_format],
                outputs=[code_result]
            )
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ä½¿ç”¨è¯´æ˜
            
            ### 1. æ¨¡å‹ç®¡ç†
            - é¦–æ¬¡ä½¿ç”¨éœ€è¦ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
            - æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
            - åŠ è½½å®Œæˆåå¯ä»¥å¼€å§‹ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½
            
            ### 2. å›¾åƒå¯¹è¯
            - ä¸Šä¼ å›¾åƒåå¯ä»¥ä¸å…¶è¿›è¡Œå¯¹è¯
            - æ”¯æŒå¤šè½®å¯¹è¯ï¼Œä¿æŒä¸Šä¸‹æ–‡
            - å¯ä»¥è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆé•¿åº¦ã€åˆ›é€ æ€§ï¼‰
            
            ### 3. OCRè¯†åˆ«
            - ä¸Šä¼ åŒ…å«æ–‡å­—çš„å›¾åƒ
            - è‡ªåŠ¨è¯†åˆ«å¹¶æå–æ‰€æœ‰æ–‡å­—å†…å®¹
            - æ”¯æŒ32ç§è¯­è¨€è¯†åˆ«
            
            ### 4. ç©ºé—´åˆ†æ
            - åˆ†æå›¾åƒä¸­çš„ç©ºé—´å…³ç³»
            - åŒ…æ‹¬ç‰©ä½“ä½ç½®ã€è§†è§’ã€é®æŒ¡å…³ç³»ç­‰
            - é€‚ç”¨äº3Dåœºæ™¯ç†è§£
            
            ### 5. è§†è§‰ç¼–ç¨‹
            - ä»å›¾åƒç”Ÿæˆå„ç§ç±»å‹çš„ä»£ç 
            - æ”¯æŒHTMLã€CSSã€JavaScriptã€Python
            - é€‚ç”¨äºUIè®¾è®¡å’ŒåŸå‹å¼€å‘
            
            ### æ³¨æ„äº‹é¡¹
            - ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼š`/data/storage1/wulin/models/qwen3-vl-8b-instruct`
            - éœ€è¦è¶³å¤Ÿçš„å†…å­˜ï¼ˆå»ºè®®16GB+ï¼‰
            - æ”¯æŒGPUåŠ é€Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
            """)
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Qwen3-VL-8B-Instruct Webç•Œé¢...")
    
    # åˆ›å»ºç•Œé¢
    interface = create_interface()
    
    interface.queue()

    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True,             # è°ƒè¯•æ¨¡å¼
        show_error=True         # æ˜¾ç¤ºé”™è¯¯
    )

if __name__ == "__main__":
    main()
