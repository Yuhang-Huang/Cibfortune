#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-VL-8B-Instruct Gradio Webç•Œé¢
æä¾›å‹å¥½çš„Webç•Œé¢æ¥ä½¿ç”¨Qwen3-VLæ¨¡å‹
"""

import os
import gradio as gr
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from PIL import Image
import requests
from io import BytesIO
import time
import json

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class Qwen3VLGradioApp:
    """Qwen3-VL Gradioåº”ç”¨ç±»"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.model_path = "/data/storage1/wulin/models/qwen3-vl-8b-instruct"
        self.is_loaded = False
        self.chat_history = []
        
    def load_model(self, progress=gr.Progress()):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            return "âœ… æ¨¡å‹å·²ç»åŠ è½½å®Œæˆï¼"
        
        try:
            progress(0.1, desc="æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
            if not os.path.exists(self.model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}"
            
            progress(0.3, desc="åŠ è½½æ¨¡å‹...")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype="auto",
                device_map="auto"
            )
            
            progress(0.7, desc="åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            
            progress(1.0, desc="å®Œæˆï¼")
            self.is_loaded = True
            
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚"
            
        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def chat_with_image(self, image, text, history, max_tokens, temperature):
        """ä¸å›¾åƒå¯¹è¯"""
        if not self.is_loaded:
            return history, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if image is None:
            return history, "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
        
        if not text.strip():
            return history, "âŒ è¯·è¾“å…¥é—®é¢˜ï¼"
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": text},
                    ],
                }
            ]
            
            # å‡†å¤‡è¾“å…¥
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True if temperature > 0 else False
                )
            
            # å¤„ç†è¾“å‡º
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            response = output_text[0]
            
            # æ›´æ–°å†å²è®°å½•
            history.append([f"ğŸ‘¤ {text}", f"ğŸ¤– {response}"])
            
            return history, ""
            
        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            history.append([f"ğŸ‘¤ {text}", error_msg])
            return history, ""
    
    def ocr_analysis(self, image):
        """OCRæ–‡å­—è¯†åˆ«"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if image is None:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
        
        try:
            prompt = "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚å¦‚æœå›¾ç‰‡ä¸­æœ‰å¤šç§è¯­è¨€ï¼Œè¯·åˆ†åˆ«æ ‡æ³¨è¯­è¨€ç±»å‹ã€‚"
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            return f"ğŸ“ OCRè¯†åˆ«ç»“æœ:\n\n{output_text[0]}"
            
        except Exception as e:
            return f"âŒ OCRè¯†åˆ«å¤±è´¥: {str(e)}"
    
    def spatial_analysis(self, image):
        """ç©ºé—´æ„ŸçŸ¥åˆ†æ"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if image is None:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
        
        try:
            prompt = """è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ç©ºé—´å…³ç³»ï¼ŒåŒ…æ‹¬ï¼š
            1. ç‰©ä½“çš„ç›¸å¯¹ä½ç½®å…³ç³»
            2. è§†è§’å’Œè§‚å¯Ÿè§’åº¦
            3. ç‰©ä½“çš„é®æŒ¡å…³ç³»
            4. æ·±åº¦å’Œè·ç¦»æ„Ÿ
            5. ç©ºé—´å¸ƒå±€çš„æ•´ä½“æè¿°"""
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            return f"ğŸ” ç©ºé—´åˆ†æç»“æœ:\n\n{output_text[0]}"
            
        except Exception as e:
            return f"âŒ ç©ºé—´åˆ†æå¤±è´¥: {str(e)}"
    
    def visual_coding(self, image, output_format):
        """è§†è§‰ç¼–ç¨‹"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        if image is None:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒï¼"
        
        try:
            format_prompts = {
                "HTML": "è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„HTMLä»£ç ï¼ŒåŒ…æ‹¬ç»“æ„ã€æ ·å¼å’Œå¸ƒå±€ã€‚",
                "CSS": "è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„CSSæ ·å¼ä»£ç ã€‚",
                "JavaScript": "è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„JavaScriptä»£ç ã€‚",
                "Python": "è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„Pythonä»£ç ã€‚"
            }
            
            prompt = format_prompts.get(output_format, format_prompts["HTML"])
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.model.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=2048)
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            return f"ğŸ’» {output_format}ä»£ç :\n\n```{output_format.lower()}\n{output_text[0]}\n```"
            
        except Exception as e:
            return f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
        return []

# åˆ›å»ºåº”ç”¨å®ä¾‹
app = Qwen3VLGradioApp()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen3-VL-8B-Instruct Webç•Œé¢",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .chat-message {
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
        }
        """
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¤– Qwen3-VL-8B-Instruct Webç•Œé¢
        
        æ¬¢è¿ä½¿ç”¨Qwen3-VL-8B-Instructå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼è¿™ä¸ªç•Œé¢æä¾›äº†å‹å¥½çš„Webäº¤äº’æ–¹å¼ã€‚
        
        **ä¸»è¦åŠŸèƒ½ï¼š**
        - ğŸ–¼ï¸ å›¾åƒç†è§£å’Œå¯¹è¯
        - ğŸ“ OCRæ–‡å­—è¯†åˆ«
        - ğŸ” ç©ºé—´æ„ŸçŸ¥åˆ†æ
        - ğŸ’» è§†è§‰ç¼–ç¨‹ï¼ˆç”Ÿæˆä»£ç ï¼‰
        """)
        
        with gr.Tab("ğŸš€ æ¨¡å‹ç®¡ç†"):
            gr.Markdown("### æ¨¡å‹åŠ è½½")
            with gr.Row():
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
                status_text = gr.Textbox(
                    label="çŠ¶æ€", 
                    value="â³ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç‚¹å‡»åŠ è½½æ¨¡å‹æŒ‰é’®",
                    interactive=False
                )
            
            load_btn.click(
                app.load_model,
                outputs=[status_text]
            )
        
        with gr.Tab("ğŸ’¬ å›¾åƒå¯¹è¯"):
            gr.Markdown("### ä¸å›¾åƒè¿›è¡Œå¯¹è¯")
            
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ å›¾åƒ",
                        type="pil",
                        height=400
                    )
                    
                    with gr.Row():
                        max_tokens = gr.Slider(
                            minimum=50, maximum=1024, value=256,
                            label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                        )
                        temperature = gr.Slider(
                            minimum=0.1, maximum=2.0, value=0.7,
                            label="åˆ›é€ æ€§ (Temperature)"
                        )
                
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=400,
                        show_label=True
                    )
                    
                    with gr.Row():
                        text_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            placeholder="è¯·æè¿°è¿™å¼ å›¾ç‰‡...",
                            lines=2
                        )
                        send_btn = gr.Button("å‘é€", variant="primary")
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²")
            
            # äº‹ä»¶ç»‘å®š
            send_btn.click(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature],
                outputs=[chatbot, text_input]
            )
            
            text_input.submit(
                app.chat_with_image,
                inputs=[image_input, text_input, chatbot, max_tokens, temperature],
                outputs=[chatbot, text_input]
            )
            
            clear_btn.click(
                app.clear_history,
                outputs=[chatbot]
            )
        
        with gr.Tab("ğŸ“ OCRè¯†åˆ«"):
            gr.Markdown("### æ–‡å­—è¯†åˆ«")
            
            with gr.Row():
                with gr.Column():
                    ocr_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒè¿›è¡ŒOCRè¯†åˆ«",
                        type="pil",
                        height=300
                    )
                    ocr_btn = gr.Button("ğŸ” å¼€å§‹è¯†åˆ«", variant="primary")
                
                with gr.Column():
                    ocr_result = gr.Textbox(
                        label="è¯†åˆ«ç»“æœ",
                        lines=15,
                        max_lines=20
                    )
            
            ocr_btn.click(
                app.ocr_analysis,
                inputs=[ocr_image],
                outputs=[ocr_result]
            )
        
        with gr.Tab("ğŸ” ç©ºé—´åˆ†æ"):
            gr.Markdown("### ç©ºé—´æ„ŸçŸ¥åˆ†æ")
            
            with gr.Row():
                with gr.Column():
                    spatial_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒè¿›è¡Œç©ºé—´åˆ†æ",
                        type="pil",
                        height=300
                    )
                    spatial_btn = gr.Button("ğŸ” å¼€å§‹åˆ†æ", variant="primary")
                
                with gr.Column():
                    spatial_result = gr.Textbox(
                        label="åˆ†æç»“æœ",
                        lines=15,
                        max_lines=20
                    )
            
            spatial_btn.click(
                app.spatial_analysis,
                inputs=[spatial_image],
                outputs=[spatial_result]
            )
        
        with gr.Tab("ğŸ’» è§†è§‰ç¼–ç¨‹"):
            gr.Markdown("### ä»å›¾åƒç”Ÿæˆä»£ç ")
            
            with gr.Row():
                with gr.Column():
                    code_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒç”Ÿæˆä»£ç ",
                        type="pil",
                        height=300
                    )
                    
                    code_format = gr.Dropdown(
                        choices=["HTML", "CSS", "JavaScript", "Python"],
                        value="HTML",
                        label="é€‰æ‹©ä»£ç ç±»å‹"
                    )
                    
                    code_btn = gr.Button("ğŸ’» ç”Ÿæˆä»£ç ", variant="primary")
                
                with gr.Column():
                    code_result = gr.Textbox(
                        label="ç”Ÿæˆçš„ä»£ç ",
                        lines=15,
                        max_lines=20
                    )
            
            code_btn.click(
                app.visual_coding,
                inputs=[code_image, code_format],
                outputs=[code_result]
            )
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ä½¿ç”¨è¯´æ˜
            
            ### 1. æ¨¡å‹ç®¡ç†
            - é¦–æ¬¡ä½¿ç”¨éœ€è¦ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
            - æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
            - åŠ è½½å®Œæˆåå¯ä»¥å¼€å§‹ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½
            
            ### 2. å›¾åƒå¯¹è¯
            - ä¸Šä¼ å›¾åƒåå¯ä»¥ä¸å…¶è¿›è¡Œå¯¹è¯
            - æ”¯æŒå¤šè½®å¯¹è¯ï¼Œä¿æŒä¸Šä¸‹æ–‡
            - å¯ä»¥è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆé•¿åº¦ã€åˆ›é€ æ€§ï¼‰
            
            ### 3. OCRè¯†åˆ«
            - ä¸Šä¼ åŒ…å«æ–‡å­—çš„å›¾åƒ
            - è‡ªåŠ¨è¯†åˆ«å¹¶æå–æ‰€æœ‰æ–‡å­—å†…å®¹
            - æ”¯æŒ32ç§è¯­è¨€è¯†åˆ«
            
            ### 4. ç©ºé—´åˆ†æ
            - åˆ†æå›¾åƒä¸­çš„ç©ºé—´å…³ç³»
            - åŒ…æ‹¬ç‰©ä½“ä½ç½®ã€è§†è§’ã€é®æŒ¡å…³ç³»ç­‰
            - é€‚ç”¨äº3Dåœºæ™¯ç†è§£
            
            ### 5. è§†è§‰ç¼–ç¨‹
            - ä»å›¾åƒç”Ÿæˆå„ç§ç±»å‹çš„ä»£ç 
            - æ”¯æŒHTMLã€CSSã€JavaScriptã€Python
            - é€‚ç”¨äºUIè®¾è®¡å’ŒåŸå‹å¼€å‘
            
            ### æ³¨æ„äº‹é¡¹
            - ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼š`/data/storage1/wulin/models/qwen3-vl-8b-instruct`
            - éœ€è¦è¶³å¤Ÿçš„å†…å­˜ï¼ˆå»ºè®®16GB+ï¼‰
            - æ”¯æŒGPUåŠ é€Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
            """)
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Qwen3-VL-8B-Instruct Webç•Œé¢...")
    
    # åˆ›å»ºç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True,             # è°ƒè¯•æ¨¡å¼
        show_error=True         # æ˜¾ç¤ºé”™è¯¯
    )

if __name__ == "__main__":
    main()
