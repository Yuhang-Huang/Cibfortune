#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¡è¯OCRè¯†åˆ« - RAGå¢å¼º + Qwen3-VL APIè°ƒç”¨
å…ˆè¿›è¡Œå¤šæ¨¡æ€RAGæ£€ç´¢å¢å¼ºï¼Œå†è°ƒç”¨Qwen3-VLå¤§æ¨¡å‹APIè·å–è¯†åˆ«ç»“æœ
"""

import os
import time
import base64
import numpy as np
from PIL import Image
from io import BytesIO
from typing import Optional, Dict, List, Tuple

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    print("è¯·å®‰è£…openai: pip install openai")
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥RAGç›¸å…³æ¨¡å—ï¼ˆæ”¯æŒå¤šç§å¯¼å…¥æ–¹å¼ï¼‰
RAG_AVAILABLE = False
MultiModalDocumentLoader = None
MultiModalVectorStore = None

# æ–¹å¼1: ä» multimodal_rag å¯¼å…¥
try:
    from multimodal_rag import MultiModalDocumentLoader, MultiModalVectorStore
    RAG_AVAILABLE = True
except ImportError:
    # æ–¹å¼2: ä» api å¯¼å…¥ï¼ˆå¦‚æœ api.py åŒ…å« multimodal_rag çš„å†…å®¹ï¼‰
    try:
        import api
        MultiModalDocumentLoader = api.MultiModalDocumentLoader
        MultiModalVectorStore = api.MultiModalVectorStore
        RAG_AVAILABLE = True
    except (ImportError, AttributeError):
        # æ–¹å¼3: ä½¿ç”¨æ ·å¼ç‰¹å¾RAGï¼ˆæ¨èï¼ŒåŸºäºé¢œè‰²ã€å¸ƒå±€ã€è¾¹ç¼˜ï¼Œæ— éœ€torchï¼‰
        # åªè¦numpyå’ŒPILå¯ç”¨å³å¯ï¼Œopencvå¯é€‰
        try:
            import numpy as np
            from PIL import Image
            RAG_AVAILABLE = True
            print("ä½¿ç”¨æ ·å¼ç‰¹å¾RAGåŠŸèƒ½ï¼ˆåŸºäºé¢œè‰²ã€å¸ƒå±€ã€è¾¹ç¼˜ï¼Œæ— éœ€torchï¼‰")
        except ImportError:
            # æ–¹å¼4: ä½¿ç”¨CLIPæ¨¡å‹ï¼ˆéœ€è¦transformerså’Œtorchï¼‰
            try:
                from transformers import CLIPProcessor, CLIPModel
                import torch
                RAG_AVAILABLE = True
                print("ä½¿ç”¨ç®€åŒ–ç‰ˆRAGåŠŸèƒ½ï¼ˆCLIPæ¨¡å‹ï¼‰")
            except ImportError:
                print("è­¦å‘Š: RAGåŠŸèƒ½å°†ä¸å¯ç”¨ï¼ˆéœ€è¦å®‰è£…numpyã€PILæˆ–transformersæ¨¡å—ï¼‰")
                RAG_AVAILABLE = False


# æ ·å¼ç‰¹å¾æå–å™¨ï¼ˆåŸºäºé¢œè‰²ã€å¸ƒå±€ã€è¾¹ç¼˜ç­‰ï¼‰
class StyleFeatureExtractor:
    """æå–å¡è¯çš„æ ·å¼ç‰¹å¾ï¼ˆé¢œè‰²ã€å¸ƒå±€ã€è¾¹ç¼˜ç­‰ï¼‰"""
    
    def __init__(self):
        try:
            import cv2
            self.cv2 = cv2
            self.use_cv2 = True
        except ImportError:
            self.use_cv2 = False
            print("âš ï¸ opencv-pythonæœªå®‰è£…ï¼Œæ ·å¼ç‰¹å¾æå–åŠŸèƒ½å—é™")
    
    def extract_style_features(self, image: Image.Image) -> np.ndarray:
        """
        æå–å›¾ç‰‡çš„æ ·å¼ç‰¹å¾
        
        Args:
            image: PIL Imageå¯¹è±¡
            
        Returns:
            æ ·å¼ç‰¹å¾å‘é‡ï¼ˆnumpyæ•°ç»„ï¼‰
        """
        features = []
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if self.use_cv2:
            img_array = np.array(image.convert('RGB'))
            img_bgr = img_array[:, :, ::-1]  # RGB to BGR for OpenCV
        else:
            img_array = np.array(image.convert('RGB'))
        
        # 1. é¢œè‰²ç›´æ–¹å›¾ç‰¹å¾ï¼ˆHSVè‰²å½©ç©ºé—´ï¼Œæ›´èƒ½åæ˜ å¡é¢é¢œè‰²é£æ ¼ï¼‰
        color_feature_size = 150  # 50*3 = 150
        try:
            if self.use_cv2:
                hsv = self.cv2.cvtColor(img_bgr, self.cv2.COLOR_BGR2HSV)
                # H(è‰²è°ƒ), S(é¥±å’Œåº¦), V(æ˜åº¦) ç›´æ–¹å›¾
                hist_h = self.cv2.calcHist([hsv], [0], None, [50], [0, 180]).flatten()
                hist_s = self.cv2.calcHist([hsv], [1], None, [50], [0, 256]).flatten()
                hist_v = self.cv2.calcHist([hsv], [2], None, [50], [0, 256]).flatten()
                # å½’ä¸€åŒ–
                hist_h = hist_h / (hist_h.sum() + 1e-8)
                hist_s = hist_s / (hist_s.sum() + 1e-8)
                hist_v = hist_v / (hist_v.sum() + 1e-8)
                features.extend(hist_h)
                features.extend(hist_s)
                features.extend(hist_v)
            else:
                # ä½¿ç”¨PILè®¡ç®—RGBç›´æ–¹å›¾
                hist_r = np.histogram(img_array[:, :, 0], bins=50, range=(0, 256))[0]
                hist_g = np.histogram(img_array[:, :, 1], bins=50, range=(0, 256))[0]
                hist_b = np.histogram(img_array[:, :, 2], bins=50, range=(0, 256))[0]
                # å½’ä¸€åŒ–
                hist_r = hist_r / (hist_r.sum() + 1e-8)
                hist_g = hist_g / (hist_g.sum() + 1e-8)
                hist_b = hist_b / (hist_b.sum() + 1e-8)
                features.extend(hist_r)
                features.extend(hist_g)
                features.extend(hist_b)
        except Exception as e:
            print(f"âš ï¸ é¢œè‰²ç‰¹å¾æå–å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼ç¡®ä¿ç»´åº¦ä¸€è‡´
            features.extend([0.0] * color_feature_size)
        
        # 2. è¾¹ç¼˜ç‰¹å¾ï¼ˆåæ˜ å¡é¢è¾¹æ¡†å’Œå¸ƒå±€ï¼‰
        edge_feature_size = 9  # 3x3 = 9
        try:
            if self.use_cv2:
                gray = self.cv2.cvtColor(img_bgr, self.cv2.COLOR_BGR2GRAY)
                edges = self.cv2.Canny(gray, 50, 150)
                # è¾¹ç¼˜å¯†åº¦ï¼ˆåˆ†æˆ9ä¸ªåŒºåŸŸï¼‰
                h, w = edges.shape
                h_step, w_step = h // 3, w // 3
                edge_densities = []
                for i in range(3):
                    for j in range(3):
                        region = edges[i*h_step:(i+1)*h_step, j*w_step:(j+1)*w_step]
                        density = np.sum(region > 0) / (region.size + 1e-8)
                        edge_densities.append(density)
                features.extend(edge_densities)
            else:
                # ä½¿ç”¨PILçš„ç®€å•è¾¹ç¼˜æ£€æµ‹
                from PIL import ImageFilter
                edges = image.convert('L').filter(ImageFilter.FIND_EDGES)
                edge_array = np.array(edges)
                # ç®€åŒ–ç‰ˆè¾¹ç¼˜å¯†åº¦
                h, w = edge_array.shape
                h_step, w_step = h // 3, w // 3
                edge_densities = []
                for i in range(3):
                    for j in range(3):
                        region = edge_array[i*h_step:(i+1)*h_step, j*w_step:(j+1)*w_step]
                        density = np.sum(region > 128) / (region.size + 1e-8)
                        edge_densities.append(density)
                features.extend(edge_densities)
        except Exception as e:
            print(f"âš ï¸ è¾¹ç¼˜ç‰¹å¾æå–å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼ç¡®ä¿ç»´åº¦ä¸€è‡´
            features.extend([0.0] * edge_feature_size)
        
        # 3. ä¸»è¦é¢œè‰²ç‰¹å¾ï¼ˆæå–å¡é¢ä¸»è‰²è°ƒï¼‰
        try:
            # ä½¿ç”¨K-meansæå–ä¸»è¦é¢œè‰²ï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥é‡‡æ ·ï¼‰
            # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ ¼å¼
            img_rgb = image.convert('RGB')
            img_resized = img_rgb.resize((100, 100))
            img_array = np.array(img_resized)
            
            # æ£€æŸ¥æ•°ç»„å½¢çŠ¶ï¼Œç¡®ä¿æ˜¯ (height, width, 3) æ ¼å¼
            if len(img_array.shape) == 3 and img_array.shape[2] == 3:
                pixels = img_array.reshape(-1, 3)
            elif len(img_array.shape) == 2:
                # å¦‚æœæ˜¯ç°åº¦å›¾ï¼Œè½¬æ¢ä¸ºRGB
                pixels = np.stack([img_array, img_array, img_array], axis=-1).reshape(-1, 3)
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
                pixels = img_array.reshape(-1, img_array.shape[-1] if len(img_array.shape) > 2 else 1)
                if pixels.shape[1] != 3:
                    # å¦‚æœæ— æ³•è½¬æ¢ä¸º3é€šé“ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    pixels = np.array([[128, 128, 128]] * 10000)  # ä½¿ç”¨ç°è‰²ä½œä¸ºé»˜è®¤å€¼
            
            # é‡‡æ ·éƒ¨åˆ†åƒç´ 
            sample_size = min(1000, len(pixels))
            if len(pixels) > sample_size:
                indices = np.random.choice(len(pixels), sample_size, replace=False)
                pixels = pixels[indices]
            
            # è®¡ç®—ä¸»è¦é¢œè‰²ï¼ˆRGBå‡å€¼ï¼‰
            if pixels.shape[1] == 3:
                main_colors = np.mean(pixels, axis=0)
                features.extend(main_colors / 255.0)  # å½’ä¸€åŒ–åˆ°0-1
            else:
                # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                features.extend([0.5, 0.5, 0.5])  # ç°è‰²
        except Exception as e:
            print(f"âš ï¸ ä¸»è‰²ç‰¹å¾æå–å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼é¿å…ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´
            features.extend([0.5, 0.5, 0.5])  # ç°è‰²
        
        # 4. å›¾åƒå°ºå¯¸å’Œå®½é«˜æ¯”ï¼ˆåæ˜ å¡é¢æ¯”ä¾‹ï¼‰
        w, h = image.size
        aspect_ratio = h / (w + 1e-8)
        features.append(aspect_ratio)
        # å½’ä¸€åŒ–çš„å°ºå¯¸
        total_pixels = w * h
        features.append(np.log(total_pixels / 1000000.0))  # å¯¹æ•°å½’ä¸€åŒ–
        
        return np.array(features, dtype=np.float32)
    
    def compute_style_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ ·å¼ç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦
        
        Args:
            features1: ç¬¬ä¸€ä¸ªå›¾ç‰‡çš„æ ·å¼ç‰¹å¾
            features2: ç¬¬äºŒä¸ªå›¾ç‰‡çš„æ ·å¼ç‰¹å¾
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(features1, features2)
        norm1 = np.linalg.norm(features1)
        norm2 = np.linalg.norm(features2)
        denom = norm1 * norm2 + 1e-8
        similarity = float(dot_product / denom) if denom > 0 else 0.0
        # å°†ä½™å¼¦ç›¸ä¼¼åº¦ä»[-1, 1]æ˜ å°„åˆ°[0, 1]
        similarity = (similarity + 1.0) / 2.0
        return similarity


# ç®€åŒ–ç‰ˆRAGå®ç°ï¼ˆåŸºäºæ ·å¼ç‰¹å¾è€ŒéCLIPï¼‰
class SimpleRAGStore:
    """ç®€åŒ–ç‰ˆRAGå­˜å‚¨ï¼ŒåŸºäºå¡é¢æ ·å¼ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦"""
    
    def __init__(self, use_style_features=True):
        """
        åˆå§‹åŒ–RAGå­˜å‚¨
        
        Args:
            use_style_features: æ˜¯å¦ä½¿ç”¨æ ·å¼ç‰¹å¾ï¼ˆTrueï¼‰æˆ–CLIPç‰¹å¾ï¼ˆFalseï¼‰
        """
        self.use_style_features = use_style_features
        self.style_extractor = StyleFeatureExtractor() if use_style_features else None
        
        if not use_style_features:
            # ä½¿ç”¨CLIPæ¨¡å‹ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
            try:
                from transformers import CLIPProcessor, CLIPModel
                import torch
                self.torch = torch
                
                # æ£€æŸ¥torchç‰ˆæœ¬
                torch_version = torch.__version__
                print(f"æ£€æµ‹åˆ°torchç‰ˆæœ¬: {torch_version}")
                
                # æ£€æŸ¥torchç‰ˆæœ¬æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼ˆ>=2.6ï¼‰
                try:
                    from packaging import version
                    if version.parse(torch_version) < version.parse("2.6.0"):
                        print(f"âš ï¸ è­¦å‘Š: torchç‰ˆæœ¬ {torch_version} ä½äº2.6ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨æ¼æ´")
                        print("å»ºè®®å‡çº§: pip install --upgrade torch>=2.6")
                except ImportError:
                    # å¦‚æœæ²¡æœ‰packagingåº“ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ¯”è¾ƒ
                    try:
                        major, minor = map(int, torch_version.split('.')[:2])
                        if major < 2 or (major == 2 and minor < 6):
                            print(f"âš ï¸ è­¦å‘Š: torchç‰ˆæœ¬ {torch_version} å¯èƒ½ä½äº2.6ï¼Œå»ºè®®å‡çº§")
                    except:
                        pass
                
                # å°è¯•åŠ è½½CLIPæ¨¡å‹ï¼ˆtransformersä¼šè‡ªåŠ¨å°è¯•ä½¿ç”¨safetensorså¦‚æœå¯ç”¨ï¼‰
                try:
                    print(f"æ­£åœ¨åŠ è½½CLIPæ¨¡å‹")
                    self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                    # å°è¯•ä½¿ç”¨safetensorsæ ¼å¼åŠ è½½ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                    try:
                        # ä½¿ç”¨use_safetensorså‚æ•°ï¼ˆå¦‚æœtransformersç‰ˆæœ¬æ”¯æŒï¼‰
                        self.model = CLIPModel.from_pretrained(
                            "openai/clip-vit-base-patch32",
                            use_safetensors=True,
                            low_cpu_mem_usage=True
                        )
                    except TypeError:
                        # å¦‚æœuse_safetensorså‚æ•°ä¸æ”¯æŒï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
                        # transformersä¼šè‡ªåŠ¨é€‰æ‹©safetensorså¦‚æœå¯ç”¨
                        self.model = CLIPModel.from_pretrained(
                            "openai/clip-vit-base-patch32",
                            low_cpu_mem_usage=True
                        )
                    self.model.eval()
                    
                except Exception as load_error:
                    error_str = str(load_error)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯torchç‰ˆæœ¬é—®é¢˜
                    if "torch.load" in error_str or "CVE-2025-32434" in error_str or "requires users" in error_str.lower():
                        raise ImportError(
                            f"âŒ torchç‰ˆæœ¬è¿‡ä½ï¼Œå­˜åœ¨å®‰å…¨æ¼æ´ï¼\n"
                            f"å½“å‰ç‰ˆæœ¬: {torch_version}\n"
                            f"è¯·å‡çº§torchåˆ°è‡³å°‘v2.6:\n"
                            f"  pip install --upgrade torch>=2.6\n"
                            f"æˆ–è€…ä½¿ç”¨safetensorsæ ¼å¼çš„æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚\n"
                            f"è¯¦ç»†é”™è¯¯: {error_str}"
                        )
                    else:
                        raise ImportError(f"æ— æ³•åŠ è½½CLIPæ¨¡å‹: {load_error}")
                
                print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
            except ImportError as e:
                raise ImportError(f"æ— æ³•åŠ è½½CLIPæ¨¡å‹: {e}")
        else:
            print("âœ… ä½¿ç”¨æ ·å¼ç‰¹å¾æå–ï¼ˆåŸºäºé¢œè‰²ã€å¸ƒå±€ã€è¾¹ç¼˜ï¼‰")
        
        self.image_embeddings = []  # å­˜å‚¨æ ·å¼ç‰¹å¾æˆ–CLIPåµŒå…¥
        self.image_metadatas = []
    
    def load_images_from_folder(self, folder_path):
        """ä»æ–‡ä»¶å¤¹åŠ è½½å›¾ç‰‡å¹¶ç”Ÿæˆæ ·å¼ç‰¹å¾æˆ–åµŒå…¥å‘é‡"""
        self.image_embeddings = []
        self.image_metadatas = []
        
        if not os.path.isdir(folder_path):
            print(f"âš ï¸ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return
        
        supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        image_files = []
        
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path) and any(filename.lower().endswith(fmt) for fmt in supported_formats):
                image_files.append((file_path, filename))
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ç”Ÿæˆ{'æ ·å¼ç‰¹å¾' if self.use_style_features else 'åµŒå…¥å‘é‡'}...")
        
        for file_path, filename in image_files:
            try:
                image = Image.open(file_path)
                
                if self.use_style_features:
                    # ä½¿ç”¨æ ·å¼ç‰¹å¾æå–
                    embedding = self.style_extractor.extract_style_features(image)
                else:
                    # ä½¿ç”¨CLIPæ¨¡å‹
                    inputs = self.processor(images=image, return_tensors="pt")
                    with self.torch.no_grad():
                        image_features = self.model.get_image_features(**inputs)
                    embedding = image_features.numpy().flatten()
                
                self.image_embeddings.append(embedding)
                self.image_metadatas.append({
                    "filename": filename,
                    "source": file_path,
                    "type": "image"
                })
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å›¾ç‰‡ {filename} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.image_embeddings)} å¼ å›¾ç‰‡çš„{'æ ·å¼ç‰¹å¾' if self.use_style_features else 'åµŒå…¥å‘é‡'}")
    
    def embed_image(self, image):
        """ç”Ÿæˆå›¾ç‰‡çš„æ ·å¼ç‰¹å¾æˆ–åµŒå…¥å‘é‡"""
        if self.use_style_features:
            # ä½¿ç”¨æ ·å¼ç‰¹å¾æå–
            return self.style_extractor.extract_style_features(image)
        else:
            # ä½¿ç”¨CLIPæ¨¡å‹
            inputs = self.processor(images=image, return_tensors="pt")
            with self.torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
            return image_features.numpy().flatten()
    
    def compute_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦"""
        if self.use_style_features:
            # ä½¿ç”¨æ ·å¼ç›¸ä¼¼åº¦è®¡ç®—
            return self.style_extractor.compute_style_similarity(features1, features2)
        else:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCLIPç‰¹å¾ï¼‰
            dot_product = np.dot(features1, features2)
            norm1 = np.linalg.norm(features1)
            norm2 = np.linalg.norm(features2)
            denom = norm1 * norm2 + 1e-8
            similarity = float(dot_product / denom) if denom > 0 else 0.0
            return similarity


class CardOCRWithRAG:
    """å¡è¯OCRè¯†åˆ« - å¸¦RAGå¢å¼ºçš„Qwen3-VL APIè°ƒç”¨ç±»"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "qwen-vl-plus",
        base_url: Optional[str] = None,
        rag_image_dir: str = "rag_cards",
        persist_directory: str = "./multimodal_chroma_card"
    ):
        """
        åˆå§‹åŒ–å¡è¯OCRè¯†åˆ«å™¨
        
        Args:
            api_key: Qwen APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡QWEN_API_KEYæˆ–OPENAI_API_KEYè¯»å–
            model: Qwenæ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨qwen-vl-plusï¼ˆæ”¯æŒè§†è§‰ï¼‰ï¼Œå¯é€‰qwen-vl-max, qwen-vl-plusç­‰
            base_url: Qwen APIåŸºç¡€URLï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„å…¼å®¹æ¨¡å¼ç«¯ç‚¹
            rag_image_dir: RAGå›¾ç‰‡åº“ç›®å½•è·¯å¾„
            persist_directory: RAGå‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
        """
        # Qwen APIé…ç½®
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„api_keyï¼Œç„¶åç¯å¢ƒå˜é‡ï¼Œæœ€åä½¿ç”¨é»˜è®¤key
        self.api_key = api_key or os.getenv("QWEN_API_KEY") or os.getenv("OPENAI_API_KEY") or "sk-c59d629c4b324848a9252e996437666b"
        self.model = model
        # Qwen API é»˜è®¤ä½¿ç”¨å…¼å®¹OpenAIæ ¼å¼çš„ç«¯ç‚¹
        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = None
        self.is_loaded = False
        
        # RAGç›¸å…³
        self.rag_image_dir = rag_image_dir
        self.persist_directory = persist_directory
        self.card_rag_store = None
        self.card_rag_ready = False
        
    def load_model(self):
        """åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯"""
        if self.is_loaded:
            print("âœ… Qwen APIå®¢æˆ·ç«¯å·²ç»åˆå§‹åŒ–")
            return True
            
        if not OPENAI_AVAILABLE:
            print("âŒ openaiåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨API")
            print("è¯·å®‰è£…: pip install openai")
            return False
            
        if not self.api_key:
            print("âŒ Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·è®¾ç½®api_keyå‚æ•°æˆ–ç¯å¢ƒå˜é‡QWEN_API_KEY")
            return False
            
        try:
            print(f"æ­£åœ¨åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯ï¼ˆæ¨¡å‹: {self.model}ï¼‰...")
            print(f"APIç«¯ç‚¹: {self.base_url}")
            
            # åˆ›å»ºOpenAIå…¼å®¹çš„å®¢æˆ·ç«¯ï¼ˆQwen APIä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼ï¼‰
            client_kwargs = {
                "api_key": self.api_key,
                "base_url": self.base_url
            }
                
            self.client = openai.OpenAI(**client_kwargs)
            self.is_loaded = True
            print("âœ… Qwen APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ Qwen APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def load_rag_library(self):
        """åŠ è½½RAGå›¾ç‰‡åº“"""
        if self.card_rag_ready:
            return self.card_rag_store is not None
            
        if not RAG_AVAILABLE:
            print("âš ï¸ RAGåŠŸèƒ½ä¸å¯ç”¨ï¼ˆéœ€è¦å®‰è£…transformersæˆ–multimodal_ragæ¨¡å—ï¼‰")
            self.card_rag_ready = True
            return False
            
        try:
            if not os.path.isdir(self.rag_image_dir):
                print(f"âš ï¸ RAGå›¾ç‰‡åº“ç›®å½•ä¸å­˜åœ¨: {self.rag_image_dir}")
                self.card_rag_ready = True
                return False
                
            print(f"æ­£åœ¨åŠ è½½RAGå›¾ç‰‡åº“: {self.rag_image_dir}")
            
            # ä¼˜å…ˆä½¿ç”¨ multimodal_rag æ¨¡å—
            if MultiModalDocumentLoader and MultiModalVectorStore:
                try:
                    loader = MultiModalDocumentLoader()
                    docs = loader.load_images_from_folder(self.rag_image_dir)
                    
                    if not docs:
                        print("âš ï¸ RAGå›¾ç‰‡åº“ä¸ºç©º")
                        self.card_rag_ready = True
                        return False
                        
                    print(f"æ‰¾åˆ° {len(docs)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
                    store = MultiModalVectorStore(persist_directory=self.persist_directory)
                    store.create_vector_store(docs)
                    self.card_rag_store = store
                    self.card_rag_ready = True
                    print(f"âœ… RAGå›¾ç‰‡åº“åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨multimodal_ragï¼‰ï¼Œå…± {len(store.image_embeddings)} å¼ å›¾ç‰‡")
                    return True
                except Exception as e:
                    print(f"âš ï¸ ä½¿ç”¨multimodal_ragåŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–ç‰ˆ: {e}")
            
            # ä½¿ç”¨ç®€åŒ–ç‰ˆRAGï¼ˆé»˜è®¤ä½¿ç”¨æ ·å¼ç‰¹å¾ï¼Œæ›´é€‚ç”¨äºå¡é¢æ ·å¼åŒ¹é…ï¼‰
            try:
                print("ä½¿ç”¨ç®€åŒ–ç‰ˆRAGåŠŸèƒ½ï¼ˆåŸºäºå¡é¢æ ·å¼ç‰¹å¾ï¼‰...")
                store = SimpleRAGStore(use_style_features=True)  # ä½¿ç”¨æ ·å¼ç‰¹å¾è€ŒéCLIP
                store.load_images_from_folder(self.rag_image_dir)
                
                if not store.image_embeddings:
                    print("âš ï¸ RAGå›¾ç‰‡åº“ä¸ºç©º")
                    self.card_rag_ready = True
                    return False
                
                self.card_rag_store = store
                self.card_rag_ready = True
                print(f"âœ… RAGå›¾ç‰‡åº“åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ç®€åŒ–ç‰ˆï¼‰ï¼Œå…± {len(store.image_embeddings)} å¼ å›¾ç‰‡")
                return True
            except ImportError as e:
                # ImportErroré€šå¸¸è¡¨ç¤ºtorchç‰ˆæœ¬é—®é¢˜æˆ–ä¾èµ–ç¼ºå¤±
                error_str = str(e)
                if "torch" in error_str.lower() or "CVE" in error_str or "version" in error_str.lower():
                    print(f"âš ï¸ ç®€åŒ–ç‰ˆRAGåŠ è½½å¤±è´¥ï¼ˆtorchç‰ˆæœ¬é—®é¢˜ï¼‰:")
                    print(f"   {error_str}")
                    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                    print("   1. å‡çº§torch: pip install --upgrade torch>=2.6")
                    print("   2. æˆ–è€…æš‚æ—¶ç¦ç”¨RAGåŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨OCRè¯†åˆ«")
                    print("   3. æˆ–è€…ä½¿ç”¨gradio_unified.pyä¸­çš„RAGåŠŸèƒ½ï¼ˆå¦‚æœå¯ç”¨ï¼‰")
                else:
                    print(f"âš ï¸ ç®€åŒ–ç‰ˆRAGåŠ è½½å¤±è´¥: {error_str}")
                self.card_rag_store = None
                self.card_rag_ready = True
                return False
            except Exception as e:
                print(f"âš ï¸ ç®€åŒ–ç‰ˆRAGåŠ è½½å¤±è´¥: {str(e)}")
                self.card_rag_store = None
                self.card_rag_ready = True
                return False
            
        except Exception as e:
            print(f"âš ï¸ RAGå›¾ç‰‡åº“åŠ è½½å¤±è´¥: {str(e)}")
            self.card_rag_store = None
            self.card_rag_ready = True
            return False
    
    def _rag_search(self, image: Image.Image, top_k: int = 3) -> List[Dict]:
        """
        å¯¹è¾“å…¥å›¾ç‰‡è¿›è¡ŒRAGæ£€ç´¢ï¼Œè¿”å›ç›¸ä¼¼å›¾ç‰‡ä¿¡æ¯
        
        Args:
            image: è¾“å…¥å›¾ç‰‡ï¼ˆPIL Imageï¼‰
            top_k: è¿”å›æœ€ç›¸ä¼¼çš„kå¼ å›¾ç‰‡
            
        Returns:
            ç›¸ä¼¼å›¾ç‰‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {filename, similarity, metadata}
        """
        if not self.card_rag_store or not hasattr(self.card_rag_store, "image_embeddings"):
            return []
            
        try:
            # ç”ŸæˆæŸ¥è¯¢å›¾ç‰‡çš„åµŒå…¥å‘é‡
            # å…¼å®¹ä¸¤ç§å®ç°ï¼šMultiModalVectorStore ä½¿ç”¨ .embeddings.embed_imageï¼ŒSimpleRAGStore ç›´æ¥ä½¿ç”¨ .embed_image
            if hasattr(self.card_rag_store, "embeddings") and hasattr(self.card_rag_store.embeddings, "embed_image"):
                # ä½¿ç”¨ MultiModalVectorStore
                query_emb = self.card_rag_store.embeddings.embed_image(image)
            elif hasattr(self.card_rag_store, "embed_image"):
                # ä½¿ç”¨ SimpleRAGStore
                query_emb = self.card_rag_store.embed_image(image)
            else:
                print("âš ï¸ RAGå­˜å‚¨ä¸æ”¯æŒembed_imageæ–¹æ³•")
                return []
            
            # è®¡ç®—ä¸å›¾ç‰‡åº“ä¸­æ‰€æœ‰å›¾ç‰‡çš„ç›¸ä¼¼åº¦
            similarities = []
            # å¦‚æœSimpleRAGStoreæœ‰compute_similarityæ–¹æ³•ï¼Œä½¿ç”¨å®ƒï¼ˆæ”¯æŒæ ·å¼ç›¸ä¼¼åº¦ï¼‰
            use_compute_similarity = hasattr(self.card_rag_store, "compute_similarity")
            
            # ç¡®ä¿æŸ¥è¯¢å‘é‡çš„ç»´åº¦
            query_dim = len(query_emb) if hasattr(query_emb, '__len__') else query_emb.shape[0] if hasattr(query_emb, 'shape') else 0
            
            for idx, emb in enumerate(self.card_rag_store.image_embeddings):
                try:
                    # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                    emb_dim = len(emb) if hasattr(emb, '__len__') else emb.shape[0] if hasattr(emb, 'shape') else 0
                    
                    if query_dim != emb_dim:
                        # ç»´åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡æˆ–ä½¿ç”¨é»˜è®¤ç›¸ä¼¼åº¦
                        print(f"âš ï¸ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æŸ¥è¯¢å‘é‡={query_dim}, å›¾ç‰‡åº“å‘é‡={emb_dim}ï¼Œè·³è¿‡è¯¥å›¾ç‰‡")
                        continue
                    
                    if use_compute_similarity:
                        # ä½¿ç”¨æ ·å¼ç›¸ä¼¼åº¦æˆ–CLIPç›¸ä¼¼åº¦ï¼ˆæ ¹æ®SimpleRAGStoreçš„é…ç½®ï¼‰
                        similarity = self.card_rag_store.compute_similarity(query_emb, emb)
                    else:
                        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆMultiModalVectorStoreï¼‰
                        dot_product = np.dot(query_emb, emb)
                        norm_query = np.linalg.norm(query_emb)
                        norm_emb = np.linalg.norm(emb)
                        denom = norm_query * norm_emb + 1e-8
                        similarity = float(dot_product / denom) if denom > 0 else 0.0
                    similarities.append((similarity, idx))
                except Exception as e:
                    # å¦‚æœè®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™ï¼Œè·³è¿‡è¯¥å›¾ç‰‡
                    print(f"âš ï¸ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ï¼ˆå›¾ç‰‡{idx}ï¼‰: {str(e)}")
                    continue
            
            # æ’åºå¹¶å–Top-K
            similarities.sort(key=lambda x: x[0], reverse=True)
            top_results = []
            
            for sim, idx in similarities[:top_k]:
                if idx < len(self.card_rag_store.image_metadatas):
                    meta = self.card_rag_store.image_metadatas[idx]
                    filename = meta.get("filename") or os.path.basename(meta.get("source", "")) or f"å›¾ç‰‡{idx+1}"
                    top_results.append({
                        "filename": filename,
                        "similarity": sim,
                        "metadata": meta
                    })
                    
            return top_results
            
        except Exception as e:
            print(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _image_to_base64(self, image: Image.Image, format: str = "PNG") -> str:
        """
        å°†PIL Imageè½¬æ¢ä¸ºbase64ç¼–ç çš„å­—ç¬¦ä¸²
        
        Args:
            image: PIL Imageå¯¹è±¡
            format: å›¾ç‰‡æ ¼å¼ï¼Œé»˜è®¤PNG
            
        Returns:
            base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²ï¼ˆdata URIæ ¼å¼ï¼‰
        """
        buffer = BytesIO()
        image.save(buffer, format=format)
        img_bytes = buffer.getvalue()
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        
        # æ ¹æ®æ ¼å¼ç¡®å®šMIMEç±»å‹
        mime_types = {
            "PNG": "image/png",
            "JPEG": "image/jpeg",
            "JPG": "image/jpeg",
            "WEBP": "image/webp"
        }
        mime_type = mime_types.get(format.upper(), "image/png")
        
        return f"data:{mime_type};base64,{img_base64}"
    
    def _build_enhanced_prompt(
        self,
        base_prompt: str,
        rag_results: List[Dict],
        custom_prompt: Optional[str] = None
    ) -> str:
        """
        æ„å»ºå¢å¼ºåçš„æç¤ºè¯ï¼ˆåŒ…å«RAGæ£€ç´¢ç»“æœï¼‰
        
        Args:
            base_prompt: åŸºç¡€æç¤ºè¯
            rag_results: RAGæ£€ç´¢ç»“æœ
            custom_prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
            
        Returns:
            å¢å¼ºåçš„å®Œæ•´æç¤ºè¯
        """
        if custom_prompt:
            prompt = custom_prompt
        else:
            prompt = base_prompt
            
        # å¦‚æœæœ‰RAGæ£€ç´¢ç»“æœï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if rag_results:
            rag_context = "\nåŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ï¼š\n"
            for rank, result in enumerate(rag_results, 1):
                filename = result["filename"]
                similarity = result["similarity"]
                rag_context += f"- å¡é¢{rank}: {filename} | ç›¸ä¼¼åº¦={similarity:.3f}\n"
            rag_context += "\n"
            filenames = [result["filename"].split(".")[0] for result in rag_results]
            banks = [filename.split("_")[0] for filename in filenames]
            prompt = rag_context + prompt
            prompt = prompt+ (
                f"6. å¦‚æœæ˜¯é“¶è¡Œå¡ä¸”å­—æ®µåˆ—è¡¨åŒ…å«'å¡é¢ç±»å‹'ï¼Œåˆ™æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å¡«å……ï¼š\n"
                f"  - åŸºäºå›¾ç‰‡åº“æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¡è¯ç»“æœ{filenames}ï¼Œå¡«å……â€œå¡é¢ç±»å‹â€å­—æ®µã€‚å­—æ®µå€¼è§„åˆ™å¦‚ä¸‹ï¼š\n"
                f"       -**ç¦æ­¢**è‡ªå®šä¹‰ã€ç”Ÿæˆã€çŒœæµ‹æˆ–ç¼–é€ æ–°çš„å¡é¢ç±»å‹å€¼ã€‚\n"
                f"       -å½“å‡ºç°ä»»ä½•ä¸ç¡®å®šã€æ¨¡ç³Šæˆ–ä¸åŒ¹é…æƒ…å†µæ—¶ï¼Œâ€œå¡é¢ç±»å‹â€å­—æ®µçš„å€¼**å¿…é¡»ä¸”åªèƒ½ä¸ºâ€œå…¶ä»–â€**ã€‚\n"
                f"       -è‹¥è¯†åˆ«å‡ºçš„â€œå‘å¡è¡Œâ€å­—æ®µçš„å€¼å­˜åœ¨ä¸{banks}ä¸­é“¶è¡Œåç§°ç›¸åŒçš„æƒ…å†µï¼Œ"
                f"åˆ™â€œå¡é¢ç±»å‹â€å­—æ®µçš„å€¼åªèƒ½ä»{filenames}ä¸­**ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ª**ã€‚\n"

            )
            
        return prompt
    
    def recognize_card(
        self,
        image: Image.Image,
        custom_prompt: Optional[str],
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8,
        use_rag: bool = True,
        top_k_rag: int = 3
    ) -> Dict:
        """
        è¯†åˆ«å¡è¯å›¾ç‰‡
        
        Args:
            image: è¾“å…¥å›¾ç‰‡ï¼ˆPIL Imageï¼‰
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰
            top_p: top_pé‡‡æ ·å‚æ•°ï¼ˆ0.0-1.0ï¼‰
            use_rag: æ˜¯å¦ä½¿ç”¨RAGå¢å¼º
            top_k_rag: RAGæ£€ç´¢è¿”å›çš„ç›¸ä¼¼å›¾ç‰‡æ•°é‡
            
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - result: OCRè¯†åˆ«ç»“æœæ–‡æœ¬
            - rag_info: RAGæ£€ç´¢ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            - generation_time: ç”Ÿæˆè€—æ—¶
            - success: æ˜¯å¦æˆåŠŸ
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        if not self.is_loaded:
            return {
                "success": False,
                "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()",
                "result": None,
                "rag_info": None,
                "generation_time": 0
            }
        
        # é»˜è®¤æç¤ºè¯
        # default_prompt = (
        #     "ä½ æ˜¯ä¸“ä¸šçš„å¡è¯OCRå¼•æ“ã€‚è¯·å¯¹å›¾ç‰‡è¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼š\n"
        #     "1) åˆ¤æ–­å¡è¯ç±»å‹ï¼ˆèº«ä»½è¯/é“¶è¡Œå¡/é©¾é©¶è¯/æŠ¤ç…§/å·¥ç‰Œ/å…¶ä»–ï¼‰ï¼›\n"
        #     "2) ä»¥Markdownè¡¨æ ¼è¾“å‡ºå…³é”®å­—æ®µå’Œå€¼ï¼›å­—æ®µç¤ºä¾‹ï¼šå§“å/å§“å(EN)ã€æ€§åˆ«ã€æ°‘æ—ã€ç”Ÿæ—¥ã€ä½å€ã€å…¬æ°‘èº«ä»½å·ç ã€ç­¾å‘æœºå…³ã€æœ‰æ•ˆæœŸé™ã€å¡å·ã€æœ‰æ•ˆæœŸã€å‘å¡è¡Œç­‰ï¼Œå¡å·ä¸­åªèƒ½åŒ…å«æ•°å­—ï¼›\n"
        #     "3) è‹¥æœ‰å¤´åƒæˆ–æ°´å°ä¿¡æ¯ï¼Œè¯·åœ¨è¡¨æ ¼ä¸‹æ–¹ä»¥æ–‡æœ¬è¡¥å……è¯´æ˜ï¼›\n"
        #     "4) ä¿æŒåŸå›¾æ–‡å­—å†…å®¹å°½é‡å®Œæ•´ï¼Œä¸è¦è¾“å‡ºå›´æ ä»£ç å—ï¼›\n"
        #     "5) å¦‚æœå’Œç»™å®šçš„å¡è¯å›¾ç‰‡åº“ä¸­çš„å›¾ç‰‡ç›¸ä¼¼ï¼Œè¯·åœ¨è¡¨æ ¼ä¸‹æ–¹ç»™å‡ºç›¸ä¼¼åº¦ï¼Œå¹¶ç»™å‡ºç›¸ä¼¼å¡è¯çš„å›¾ç‰‡åç§°ã€‚"
        # )
        default_prompt = None

        # RAGæ£€ç´¢
        rag_results = []
        if use_rag and self.card_rag_store:
            rag_results = self._rag_search(image, top_k=top_k_rag)
        
        # æ„å»ºå¢å¼ºæç¤ºè¯
        enhanced_prompt = self._build_enhanced_prompt(
            custom_prompt,
            rag_results,
            default_prompt
        )
        
        # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64
        image_base64 = self._image_to_base64(image)
        
        # åœ¨ç»ˆç«¯è¾“å‡ºå‘é€ç»™APIçš„å®Œæ•´prompt
        print("\n" + "=" * 80)
        print("ğŸ“ å‘é€ç»™APIçš„å®Œæ•´Prompt")
        print("=" * 80)
        print(enhanced_prompt)
        print("=" * 80 + "\n")
        
        # å‡†å¤‡Qwen APIæ¶ˆæ¯æ ¼å¼ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_base64
                        }
                    },
                    {
                        "type": "text",
                        "text": enhanced_prompt
                    }
                ]
            }
        ]
        
        # è°ƒç”¨Qwen API
        try:
            start_time = time.time()
            
            # å‡†å¤‡APIå‚æ•°
            api_params = {
                "model": self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
            }
            
            # top_på‚æ•°ï¼šå¦‚æœå°äº1.0åˆ™æ·»åŠ ï¼Œå¦åˆ™ä¸ä¼ ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
            if top_p < 1.0:
                api_params["top_p"] = top_p
            
            response = self.client.chat.completions.create(**api_params)
            
            generation_time = time.time() - start_time
            
            # æå–å“åº”æ–‡æœ¬
            result_text = response.choices[0].message.content
            
            # æ„å»ºRAGä¿¡æ¯
            rag_info = None
            if rag_results:
                rag_info = {
                    "enabled": True,
                    "top_k": len(rag_results),
                    "results": rag_results
                }
            else:
                rag_info = {"enabled": False, "reason": "RAGæœªå¯ç”¨æˆ–å›¾ç‰‡åº“ä¸ºç©º"}
            
            return {
                "success": True,
                "result": result_text,
                "rag_info": rag_info,
                "generation_time": generation_time,
                "error": None
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "result": None,
                "rag_info": None,
                "generation_time": 0
            }
    
    def recognize_from_file(
        self,
        image_path: str,
        **kwargs
    ) -> Dict:
        """
        ä»æ–‡ä»¶è·¯å¾„è¯†åˆ«å¡è¯
        
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            **kwargs: ä¼ é€’ç»™recognize_cardçš„å…¶ä»–å‚æ•°
            
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        try:
            image = Image.open(image_path)
            return self.recognize_card(image, **kwargs)
        except Exception as e:
            return {
                "success": False,
                "error": f"å›¾ç‰‡åŠ è½½å¤±è´¥: {str(e)}",
                "result": None,
                "rag_info": None,
                "generation_time": 0
            }


def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    print("=" * 60)
    print("å¡è¯OCRè¯†åˆ« - RAGå¢å¼º + Qwen3-VL APIè°ƒç”¨")
    print("=" * 60)
    
    # åˆ›å»ºè¯†åˆ«å™¨å®ä¾‹ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤API keyï¼‰
    ocr = CardOCRWithRAG(
        api_key=None,  # å¦‚æœä¸ºNoneï¼Œä¼šä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤key
        model="qwen-vl-plus",  # æˆ–ä½¿ç”¨ "qwen-vl-max", "qwen-vl-max-longcontext"
        rag_image_dir="rag_cards",
        persist_directory="./multimodal_chroma_card"
    )
    
    print(f"ä½¿ç”¨APIå¯†é’¥: {ocr.api_key[:10]}...")
    
    # åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯
    print("\n1. åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯...")
    if not ocr.load_model():
        print("âŒ Qwen APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # åŠ è½½RAGå›¾ç‰‡åº“ï¼ˆå¯é€‰ï¼‰
    print("\n2. åŠ è½½RAGå›¾ç‰‡åº“...")
    ocr.load_rag_library()
    
    # ç¤ºä¾‹ï¼šè¯†åˆ«å¡è¯å›¾ç‰‡
    print("\n3. å¼€å§‹è¯†åˆ«...")
    test_image_path = input("è¯·è¾“å…¥å¡è¯å›¾ç‰‡è·¯å¾„ï¼ˆæˆ–æŒ‰Enterè·³è¿‡ï¼‰: ").strip()
    
    if not test_image_path:
        print("è·³è¿‡æµ‹è¯•")
        return
        
    if not os.path.exists(test_image_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {test_image_path}")
        return
    
    # æ‰§è¡Œè¯†åˆ«
    result = ocr.recognize_from_file(test_image_path, use_rag=True)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("è¯†åˆ«ç»“æœ")
    print("=" * 60)
    
    if result["success"]:
        print(f"\nâœ… è¯†åˆ«æˆåŠŸï¼ˆè€—æ—¶: {result['generation_time']:.2f}ç§’ï¼‰")
        print(f"\nè¯†åˆ«ç»“æœ:\n{result['result']}")
        
        if result["rag_info"] and result["rag_info"]["enabled"]:
            print(f"\nğŸ“Š RAGæ£€ç´¢ä¿¡æ¯:")
            print(f"  æ‰¾åˆ° {result['rag_info']['top_k']} å¼ ç›¸ä¼¼å›¾ç‰‡")
            for i, r in enumerate(result["rag_info"]["results"], 1):
                print(f"  {i}. {r['filename']} (ç›¸ä¼¼åº¦: {r['similarity']:.3f})")
    else:
        print(f"\nâŒ è¯†åˆ«å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    main()

